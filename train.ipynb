{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d53434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:20:09.332478Z",
     "iopub.status.busy": "2023-11-16T02:20:09.332133Z",
     "iopub.status.idle": "2023-11-16T02:22:47.987336Z",
     "shell.execute_reply": "2023-11-16T02:22:47.986352Z"
    },
    "papermill": {
     "duration": 158.67367,
     "end_time": "2023-11-16T02:22:47.989625",
     "exception": false,
     "start_time": "2023-11-16T02:20:09.315955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n",
      "Collecting torchgeometry\r\n",
      "  Downloading torchgeometry-0.1.2-py2.py3-none-any.whl (42 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from torchgeometry) (2.0.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->torchgeometry) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->torchgeometry) (1.3.0)\r\n",
      "Installing collected packages: torchgeometry\r\n",
      "Successfully installed torchgeometry-0.1.2\r\n",
      "Collecting git+https://github.com/mberkay0/pretrained-backbones-unet\r\n",
      "  Cloning https://github.com/mberkay0/pretrained-backbones-unet to /tmp/pip-req-build-mporgmld\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/mberkay0/pretrained-backbones-unet /tmp/pip-req-build-mporgmld\r\n",
      "  Resolved https://github.com/mberkay0/pretrained-backbones-unet to commit c2e9bea6caef0883f1f34d40ecaebb8c374e39e0\r\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting torch==1.13.1 (from pretrained-backbones-unet==0.0.1)\r\n",
      "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m774.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/timm/\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting timm==0.6.12 (from pretrained-backbones-unet==0.0.1)\r\n",
      "  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm==0.6.12->pretrained-backbones-unet==0.0.1) (0.15.1)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.6.12->pretrained-backbones-unet==0.0.1) (6.0.1)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.6.12->pretrained-backbones-unet==0.0.1) (0.17.3)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==1.13.1->pretrained-backbones-unet==0.0.1) (4.5.0)\r\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1->pretrained-backbones-unet==0.0.1)\r\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1->pretrained-backbones-unet==0.0.1)\r\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1->pretrained-backbones-unet==0.0.1)\r\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1->pretrained-backbones-unet==0.0.1)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->pretrained-backbones-unet==0.0.1) (68.1.2)\r\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->pretrained-backbones-unet==0.0.1) (0.41.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.6.12->pretrained-backbones-unet==0.0.1) (3.12.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.6.12->pretrained-backbones-unet==0.0.1) (2023.10.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.6.12->pretrained-backbones-unet==0.0.1) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.6.12->pretrained-backbones-unet==0.0.1) (4.66.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.6.12->pretrained-backbones-unet==0.0.1) (21.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm==0.6.12->pretrained-backbones-unet==0.0.1) (1.24.3)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm==0.6.12->pretrained-backbones-unet==0.0.1) (10.1.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.6.12->pretrained-backbones-unet==0.0.1) (3.0.9)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.6.12->pretrained-backbones-unet==0.0.1) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.6.12->pretrained-backbones-unet==0.0.1) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.6.12->pretrained-backbones-unet==0.0.1) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.6.12->pretrained-backbones-unet==0.0.1) (2023.7.22)\r\n",
      "Building wheels for collected packages: pretrained-backbones-unet\r\n",
      "  Building wheel for pretrained-backbones-unet (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretrained-backbones-unet: filename=pretrained_backbones_unet-0.0.1-py3-none-any.whl size=15687 sha256=70ddc669048d0db247bb57409bdae18df31b5bee07466961a8ec175385345876\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-s_kmqp1e/wheels/bb/dd/68/01723e99f4d80833adcb02f29f3eed11ea8c1046b5811d352e\r\n",
      "Successfully built pretrained-backbones-unet\r\n",
      "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, timm, pretrained-backbones-unet\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.0.0\r\n",
      "    Uninstalling torch-2.0.0:\r\n",
      "      Successfully uninstalled torch-2.0.0\r\n",
      "  Attempting uninstall: timm\r\n",
      "    Found existing installation: timm 0.9.10\r\n",
      "    Uninstalling timm-0.9.10:\r\n",
      "      Successfully uninstalled timm-0.9.10\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pretrained-backbones-unet-0.0.1 timm-0.6.12 torch-1.13.1\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchsummary\n",
    "# !pip install torchgeometry\n",
    "# !pip install git+https://github.com/mberkay0/pretrained-backbones-unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b347c38f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-16T02:22:48.143232Z",
     "iopub.status.busy": "2023-11-16T02:22:48.142373Z",
     "iopub.status.idle": "2023-11-16T02:22:54.287318Z",
     "shell.execute_reply": "2023-11-16T02:22:54.286474Z"
    },
    "papermill": {
     "duration": 6.224574,
     "end_time": "2023-11-16T02:22:54.289648",
     "exception": false,
     "start_time": "2023-11-16T02:22:48.065074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "from torchgeometry.losses import one_hot\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "from torchvision.transforms import *\n",
    "from collections import OrderedDict\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import wandb\n",
    "from backbones_unet.model.unet import Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "679b2abf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:22:54.446294Z",
     "iopub.status.busy": "2023-11-16T02:22:54.445391Z",
     "iopub.status.idle": "2023-11-16T02:22:55.426945Z",
     "shell.execute_reply": "2023-11-16T02:22:55.425801Z"
    },
    "papermill": {
     "duration": 1.062499,
     "end_time": "2023-11-16T02:22:55.429210",
     "exception": false,
     "start_time": "2023-11-16T02:22:54.366711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla T4 (UUID: GPU-8bbcafeb-a7e7-1fb4-7fac-c735c1928772)\r\n",
      "GPU 1: Tesla T4 (UUID: GPU-246b9022-5dd1-84e7-4164-5818c91a36f0)\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15bdee63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:22:55.580250Z",
     "iopub.status.busy": "2023-11-16T02:22:55.579933Z",
     "iopub.status.idle": "2023-11-16T02:22:55.584630Z",
     "shell.execute_reply": "2023-11-16T02:22:55.583784Z"
    },
    "papermill": {
     "duration": 0.082336,
     "end_time": "2023-11-16T02:22:55.586492",
     "exception": false,
     "start_time": "2023-11-16T02:22:55.504156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1af8fe01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:22:55.733590Z",
     "iopub.status.busy": "2023-11-16T02:22:55.733308Z",
     "iopub.status.idle": "2023-11-16T02:22:55.829159Z",
     "shell.execute_reply": "2023-11-16T02:22:55.828278Z"
    },
    "papermill": {
     "duration": 0.171343,
     "end_time": "2023-11-16T02:22:55.831014",
     "exception": false,
     "start_time": "2023-11-16T02:22:55.659671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7747639",
   "metadata": {
    "papermill": {
     "duration": 0.073527,
     "end_time": "2023-11-16T02:22:55.979969",
     "exception": false,
     "start_time": "2023-11-16T02:22:55.906442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e9ed26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:22:56.127420Z",
     "iopub.status.busy": "2023-11-16T02:22:56.127101Z",
     "iopub.status.idle": "2023-11-16T02:22:56.132769Z",
     "shell.execute_reply": "2023-11-16T02:22:56.131987Z"
    },
    "papermill": {
     "duration": 0.082384,
     "end_time": "2023-11-16T02:22:56.134670",
     "exception": false,
     "start_time": "2023-11-16T02:22:56.052286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of class in the data set (3: neoplastic, non neoplastic, background)\n",
    "num_classes = 3\n",
    "\n",
    "# Number of epoch\n",
    "epochs = 50\n",
    "\n",
    "# Hyperparameters for training \n",
    "learning_rate = 2e-04\n",
    "batch_size = 4\n",
    "display_step = 50\n",
    "\n",
    "# Model path\n",
    "checkpoint_path = '/kaggle/working/unet_model.pth'\n",
    "pretrained_path = \"/kaggle/input/unet-checkpoint/unet_model.pth\"\n",
    "# Initialize lists to keep track of loss and accuracy\n",
    "loss_epoch_array = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "valid_accuracy = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3a7c0b",
   "metadata": {
    "papermill": {
     "duration": 0.073211,
     "end_time": "2023-11-16T02:22:56.280559",
     "exception": false,
     "start_time": "2023-11-16T02:22:56.207348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19308f46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:22:56.430510Z",
     "iopub.status.busy": "2023-11-16T02:22:56.430164Z",
     "iopub.status.idle": "2023-11-16T02:22:56.437505Z",
     "shell.execute_reply": "2023-11-16T02:22:56.436660Z"
    },
    "papermill": {
     "duration": 0.085711,
     "end_time": "2023-11-16T02:22:56.439620",
     "exception": false,
     "start_time": "2023-11-16T02:22:56.353909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = Compose([Resize((512, 512), interpolation=InterpolationMode.BILINEAR),\n",
    "                     PILToTensor()])\n",
    "\n",
    "augmentation_1 = Compose([\n",
    "                        Resize((512, 512), interpolation=InterpolationMode.BILINEAR),\n",
    "                        RandomHorizontalFlip(p=1),\n",
    "                        PILToTensor()\n",
    "                        ])\n",
    "\n",
    "augmentation_2 = Compose([\n",
    "                        Resize((512, 512), interpolation=InterpolationMode.BILINEAR),\n",
    "                        RandomVerticalFlip(p=1),\n",
    "                        PILToTensor()\n",
    "                        ])\n",
    "\n",
    "augmentation_3 = Compose([\n",
    "                        Resize((512, 512), interpolation=InterpolationMode.BILINEAR),\n",
    "                        RandomVerticalFlip(p=1),\n",
    "                        RandomHorizontalFlip(p=1),\n",
    "                        PILToTensor()\n",
    "                        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85e4a049",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:22:56.589144Z",
     "iopub.status.busy": "2023-11-16T02:22:56.588850Z",
     "iopub.status.idle": "2023-11-16T02:22:56.597915Z",
     "shell.execute_reply": "2023-11-16T02:22:56.597150Z"
    },
    "papermill": {
     "duration": 0.085677,
     "end_time": "2023-11-16T02:22:56.599692",
     "exception": false,
     "start_time": "2023-11-16T02:22:56.514015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UNetDataClass(Dataset):\n",
    "    def __init__(self, images_path, masks_path, transform=None):\n",
    "        super(UNetDataClass, self).__init__()\n",
    "        \n",
    "        images_list = os.listdir(images_path)\n",
    "        masks_list = os.listdir(masks_path)\n",
    "        \n",
    "        images_list = [images_path + image_name for image_name in images_list]\n",
    "        masks_list = [masks_path + mask_name for mask_name in masks_list]\n",
    "        \n",
    "        self.images_list = images_list\n",
    "        self.masks_list = masks_list\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.images_list[index]\n",
    "        mask_path = self.masks_list[index]\n",
    "        \n",
    "        # Open image and mask\n",
    "        data = Image.open(img_path)\n",
    "        label = Image.open(mask_path)\n",
    "            \n",
    "        if (self.transform):\n",
    "            data = self.transform(data) / 255\n",
    "            label = self.transform(label) / 255\n",
    "\n",
    "        \n",
    "        label = torch.where(label>0.65, 1.0, 0.0)\n",
    "        \n",
    "        label[2, :, :] = 0.0001\n",
    "        label = torch.argmax(label, 0).type(torch.int64)\n",
    "        \n",
    "        return data, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37fed3e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:22:56.902388Z",
     "iopub.status.busy": "2023-11-16T02:22:56.902093Z",
     "iopub.status.idle": "2023-11-16T02:22:56.906110Z",
     "shell.execute_reply": "2023-11-16T02:22:56.905325Z"
    },
    "papermill": {
     "duration": 0.080389,
     "end_time": "2023-11-16T02:22:56.908141",
     "exception": false,
     "start_time": "2023-11-16T02:22:56.827752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "images_path = \"/kaggle/input/bkai-igh-neopolyp/train/train/\"\n",
    "masks_path =  \"/kaggle/input/bkai-igh-neopolyp/train_gt/train_gt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e99a62d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:22:57.058404Z",
     "iopub.status.busy": "2023-11-16T02:22:57.057809Z",
     "iopub.status.idle": "2023-11-16T02:22:57.186375Z",
     "shell.execute_reply": "2023-11-16T02:22:57.185322Z"
    },
    "papermill": {
     "duration": 0.205315,
     "end_time": "2023-11-16T02:22:57.188480",
     "exception": false,
     "start_time": "2023-11-16T02:22:56.983165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "unet_dataset_not_aug = UNetDataClass(images_path, masks_path, transform=transform)\n",
    "unet_dataset_aug_1 = UNetDataClass(images_path, masks_path, transform=augmentation_1)\n",
    "unet_dataset_aug_2 = UNetDataClass(images_path, masks_path, transform=augmentation_2)\n",
    "unet_dataset_aug_3 = UNetDataClass(images_path, masks_path, transform=augmentation_3)\n",
    "unet_dataset_1 = ConcatDataset([unet_dataset_not_aug, unet_dataset_aug_1])\n",
    "unet_dataset_2 = ConcatDataset([unet_dataset_1, unet_dataset_aug_2])\n",
    "unet_dataset = ConcatDataset([unet_dataset_2, unet_dataset_aug_3])\n",
    "# unet_dataset = unet_dataset_not_aug\n",
    "print(len(unet_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "032747a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:22:57.340273Z",
     "iopub.status.busy": "2023-11-16T02:22:57.339974Z",
     "iopub.status.idle": "2023-11-16T02:22:57.344238Z",
     "shell.execute_reply": "2023-11-16T02:22:57.343457Z"
    },
    "papermill": {
     "duration": 0.083156,
     "end_time": "2023-11-16T02:22:57.346072",
     "exception": false,
     "start_time": "2023-11-16T02:22:57.262916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "valid_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe4ce502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:22:57.500149Z",
     "iopub.status.busy": "2023-11-16T02:22:57.499312Z",
     "iopub.status.idle": "2023-11-16T02:22:57.504605Z",
     "shell.execute_reply": "2023-11-16T02:22:57.503860Z"
    },
    "papermill": {
     "duration": 0.085587,
     "end_time": "2023-11-16T02:22:57.506618",
     "exception": false,
     "start_time": "2023-11-16T02:22:57.421031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set, valid_set = random_split(unet_dataset, \n",
    "                                    [int(train_size * len(unet_dataset)) , \n",
    "                                     int(valid_size * len(unet_dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46757bca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:22:57.655204Z",
     "iopub.status.busy": "2023-11-16T02:22:57.654959Z",
     "iopub.status.idle": "2023-11-16T02:22:57.659879Z",
     "shell.execute_reply": "2023-11-16T02:22:57.659047Z"
    },
    "papermill": {
     "duration": 0.080661,
     "end_time": "2023-11-16T02:22:57.661766",
     "exception": false,
     "start_time": "2023-11-16T02:22:57.581105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f158e4",
   "metadata": {
    "papermill": {
     "duration": 0.075734,
     "end_time": "2023-11-16T02:22:59.205513",
     "exception": false,
     "start_time": "2023-11-16T02:22:59.129779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d531e2aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:22:59.355202Z",
     "iopub.status.busy": "2023-11-16T02:22:59.354868Z",
     "iopub.status.idle": "2023-11-16T02:22:59.366819Z",
     "shell.execute_reply": "2023-11-16T02:22:59.365896Z"
    },
    "papermill": {
     "duration": 0.089861,
     "end_time": "2023-11-16T02:22:59.368835",
     "exception": false,
     "start_time": "2023-11-16T02:22:59.278974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CEDiceLoss(nn.Module):\n",
    "    def __init__(self, weights) -> None:\n",
    "        super(CEDiceLoss, self).__init__()\n",
    "        self.eps: float = 1e-6\n",
    "        self.weights: torch.Tensor = weights\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input: torch.Tensor,\n",
    "            target: torch.Tensor) -> torch.Tensor:\n",
    "        if not torch.is_tensor(input):\n",
    "            raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n",
    "                            .format(type(input)))\n",
    "        if not len(input.shape) == 4:\n",
    "            raise ValueError(\"Invalid input shape, we expect BxNxHxW. Got: {}\"\n",
    "                             .format(input.shape))\n",
    "        if not input.shape[-2:] == target.shape[-2:]:\n",
    "            raise ValueError(\"input and target shapes must be the same. Got: {}\"\n",
    "                             .format(input.shape, input.shape))\n",
    "        if not input.device == target.device:\n",
    "            raise ValueError(\n",
    "                \"input and target must be in the same device. Got: {}\" .format(\n",
    "                    input.device, target.device))\n",
    "        if not self.weights.shape[1] == input.shape[1]:\n",
    "            raise ValueError(\"The number of weights must equal the number of classes\")\n",
    "        if not torch.sum(self.weights).item() == 1:\n",
    "            raise ValueError(\"The sum of all weights must equal 1\")\n",
    "            \n",
    "        # cross entropy loss\n",
    "        celoss = nn.CrossEntropyLoss(self.weights)(input, target)\n",
    "        \n",
    "        # compute softmax over the classes axis\n",
    "        input_soft = F.softmax(input, dim=1)\n",
    "\n",
    "        # create the labels one hot tensor\n",
    "        target_one_hot = one_hot(target, num_classes=input.shape[1],\n",
    "                                 device=input.device, dtype=input.dtype)\n",
    "\n",
    "        # compute the actual dice score\n",
    "        dims = (2, 3)\n",
    "        intersection = torch.sum(input_soft * target_one_hot, dims)\n",
    "        cardinality = torch.sum(input_soft + target_one_hot, dims)\n",
    "\n",
    "        dice_score = 2. * intersection / (cardinality + self.eps)\n",
    "        \n",
    "        dice_score = torch.sum(dice_score * self.weights, dim=1)\n",
    "        \n",
    "        return torch.mean(1. - dice_score) + celoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eedc87",
   "metadata": {
    "papermill": {
     "duration": 0.074168,
     "end_time": "2023-11-16T02:22:59.519527",
     "exception": false,
     "start_time": "2023-11-16T02:22:59.445359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4593c",
   "metadata": {
    "papermill": {
     "duration": 0.073594,
     "end_time": "2023-11-16T02:22:59.666936",
     "exception": false,
     "start_time": "2023-11-16T02:22:59.593342",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Initialize weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d2fa6dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:22:59.817878Z",
     "iopub.status.busy": "2023-11-16T02:22:59.817548Z",
     "iopub.status.idle": "2023-11-16T02:22:59.822142Z",
     "shell.execute_reply": "2023-11-16T02:22:59.821415Z"
    },
    "papermill": {
     "duration": 0.082127,
     "end_time": "2023-11-16T02:22:59.824102",
     "exception": false,
     "start_time": "2023-11-16T02:22:59.741975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_init(model):\n",
    "    if isinstance(model, nn.Linear):\n",
    "        # Xavier Distribution\n",
    "        torch.nn.init.xavier_uniform_(model.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4d4667b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:22:59.980833Z",
     "iopub.status.busy": "2023-11-16T02:22:59.980523Z",
     "iopub.status.idle": "2023-11-16T02:22:59.986599Z",
     "shell.execute_reply": "2023-11-16T02:22:59.985648Z"
    },
    "papermill": {
     "duration": 0.087911,
     "end_time": "2023-11-16T02:22:59.988742",
     "exception": false,
     "start_time": "2023-11-16T02:22:59.900831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, path):\n",
    "    checkpoint = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "def load_model(model, optimizer, path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62ff8a7",
   "metadata": {
    "papermill": {
     "duration": 0.073663,
     "end_time": "2023-11-16T02:23:00.141433",
     "exception": false,
     "start_time": "2023-11-16T02:23:00.067770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dbea757",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:23:00.293684Z",
     "iopub.status.busy": "2023-11-16T02:23:00.293344Z",
     "iopub.status.idle": "2023-11-16T02:23:00.304663Z",
     "shell.execute_reply": "2023-11-16T02:23:00.303916Z"
    },
    "papermill": {
     "duration": 0.08832,
     "end_time": "2023-11-16T02:23:00.306577",
     "exception": false,
     "start_time": "2023-11-16T02:23:00.218257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train function for each epoch\n",
    "def train(train_dataloader, valid_dataloader,learing_rate_scheduler, epoch, display_step):\n",
    "    print(f\"Start epoch #{epoch+1}, learning rate for this epoch: {learing_rate_scheduler.get_last_lr()}\")\n",
    "    start_time = time.time()\n",
    "    train_loss_epoch = 0\n",
    "    test_loss_epoch = 0\n",
    "    last_loss = 999999999\n",
    "    model.train()\n",
    "    for i, (data,targets) in enumerate(train_dataloader):\n",
    "        \n",
    "        # Load data into GPU\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Backpropagation, compute gradients\n",
    "        loss = loss_function(outputs, targets.long())\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Save loss\n",
    "        train_loss_epoch += loss.item()\n",
    "        if (i+1) % display_step == 0:\n",
    "#             accuracy = float(test(test_loader))\n",
    "            print('Train Epoch: {} [{}/{} ({}%)]\\tLoss: {:.4f}'.format(\n",
    "                epoch + 1, (i+1) * len(data), len(train_dataloader.dataset), 100 * (i+1) * len(data) / len(train_dataloader.dataset), \n",
    "                loss.item()))\n",
    "                  \n",
    "    print(f\"Done epoch #{epoch+1}, time for this epoch: {time.time()-start_time}s\")\n",
    "    train_loss_epoch/= (i + 1)\n",
    "    \n",
    "    # Evaluate the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            test_output = model(data)\n",
    "            test_loss = loss_function(test_output, target)\n",
    "            test_loss_epoch += test_loss.item()\n",
    "            \n",
    "    test_loss_epoch/= (i+1)\n",
    "    \n",
    "    return train_loss_epoch , test_loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ec5b9ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:23:00.620478Z",
     "iopub.status.busy": "2023-11-16T02:23:00.620136Z",
     "iopub.status.idle": "2023-11-16T02:23:04.029361Z",
     "shell.execute_reply": "2023-11-16T02:23:04.028290Z"
    },
    "papermill": {
     "duration": 3.487095,
     "end_time": "2023-11-16T02:23:04.032139",
     "exception": false,
     "start_time": "2023-11-16T02:23:00.545044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1_0-14fe96d1.pth\" to /root/.cache/torch/hub/checkpoints/resnet50_a1_0-14fe96d1.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Unet(\n",
       "    (encoder): FeatureListNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): UnetDecoder(\n",
       "      (center): Identity()\n",
       "      (blocks): ModuleList(\n",
       "        (0): DecoderBlock(\n",
       "          (conv1): Conv2dBnAct(\n",
       "            (conv): Conv2d(3072, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): Conv2dBnAct(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): DecoderBlock(\n",
       "          (conv1): Conv2dBnAct(\n",
       "            (conv): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): Conv2dBnAct(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (2): DecoderBlock(\n",
       "          (conv1): Conv2dBnAct(\n",
       "            (conv): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): Conv2dBnAct(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (3): DecoderBlock(\n",
       "          (conv1): Conv2dBnAct(\n",
       "            (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): Conv2dBnAct(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (4): DecoderBlock(\n",
       "          (conv1): Conv2dBnAct(\n",
       "            (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): Conv2dBnAct(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_conv): Conv2d(16, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Unet(\n",
    "    in_channels=3,            \n",
    "    num_classes=num_classes,      \n",
    ")\n",
    "model.apply(weights_init)\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e37d4ff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:23:04.334885Z",
     "iopub.status.busy": "2023-11-16T02:23:04.334052Z",
     "iopub.status.idle": "2023-11-16T02:23:04.341172Z",
     "shell.execute_reply": "2023-11-16T02:23:04.340452Z"
    },
    "papermill": {
     "duration": 0.086632,
     "end_time": "2023-11-16T02:23:04.343079",
     "exception": false,
     "start_time": "2023-11-16T02:23:04.256447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = torch.Tensor([[0.4, 0.55, 0.05]]).cuda()\n",
    "loss_function = CEDiceLoss(weights)\n",
    "\n",
    "# Define the optimizer (Adam optimizer)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# Learning rate scheduler\n",
    "learing_rate_scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3858525e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:23:04.501233Z",
     "iopub.status.busy": "2023-11-16T02:23:04.500396Z",
     "iopub.status.idle": "2023-11-16T02:23:04.719984Z",
     "shell.execute_reply": "2023-11-16T02:23:04.719007Z"
    },
    "papermill": {
     "duration": 0.302901,
     "end_time": "2023-11-16T02:23:04.722205",
     "exception": false,
     "start_time": "2023-11-16T02:23:04.419304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_model(model, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e174ed3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:23:05.038279Z",
     "iopub.status.busy": "2023-11-16T02:23:05.037629Z",
     "iopub.status.idle": "2023-11-16T02:23:06.475183Z",
     "shell.execute_reply": "2023-11-16T02:23:06.474394Z"
    },
    "papermill": {
     "duration": 1.518225,
     "end_time": "2023-11-16T02:23:06.477135",
     "exception": false,
     "start_time": "2023-11-16T02:23:04.958910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(\n",
    "    key = \"a561d9f8d588433211a8762b70588097e48c01df\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af188c5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T02:23:06.630318Z",
     "iopub.status.busy": "2023-11-16T02:23:06.629660Z",
     "iopub.status.idle": "2023-11-16T07:28:17.717263Z",
     "shell.execute_reply": "2023-11-16T07:28:17.716253Z"
    },
    "papermill": {
     "duration": 18311.166033,
     "end_time": "2023-11-16T07:28:17.719780",
     "exception": false,
     "start_time": "2023-11-16T02:23:06.553747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbinh-nt2003bg\u001b[0m (\u001b[33mnot_having_team_yet\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20231116_022306-87s05h66\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mjolly-music-57\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/not_having_team_yet/assignment_3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/not_having_team_yet/assignment_3/runs/87s05h66\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch #1, learning rate for this epoch: [0.0002]\n",
      "Train Epoch: 1 [200/3200 (6.25%)]\tLoss: 1.8361\n",
      "Train Epoch: 1 [400/3200 (12.5%)]\tLoss: 1.4634\n",
      "Train Epoch: 1 [600/3200 (18.75%)]\tLoss: 1.3930\n",
      "Train Epoch: 1 [800/3200 (25.0%)]\tLoss: 1.2424\n",
      "Train Epoch: 1 [1000/3200 (31.25%)]\tLoss: 1.3889\n",
      "Train Epoch: 1 [1200/3200 (37.5%)]\tLoss: 1.1769\n",
      "Train Epoch: 1 [1400/3200 (43.75%)]\tLoss: 1.2127\n",
      "Train Epoch: 1 [1600/3200 (50.0%)]\tLoss: 1.3545\n",
      "Train Epoch: 1 [1800/3200 (56.25%)]\tLoss: 1.2727\n",
      "Train Epoch: 1 [2000/3200 (62.5%)]\tLoss: 1.1528\n",
      "Train Epoch: 1 [2200/3200 (68.75%)]\tLoss: 1.1318\n",
      "Train Epoch: 1 [2400/3200 (75.0%)]\tLoss: 1.2358\n",
      "Train Epoch: 1 [2600/3200 (81.25%)]\tLoss: 1.0296\n",
      "Train Epoch: 1 [2800/3200 (87.5%)]\tLoss: 1.3660\n",
      "Train Epoch: 1 [3000/3200 (93.75%)]\tLoss: 0.9280\n",
      "Train Epoch: 1 [3200/3200 (100.0%)]\tLoss: 0.8399\n",
      "Done epoch #1, time for this epoch: 333.3135452270508s\n",
      "Start epoch #2, learning rate for this epoch: [0.0002]\n",
      "Train Epoch: 2 [200/3200 (6.25%)]\tLoss: 0.8890\n",
      "Train Epoch: 2 [400/3200 (12.5%)]\tLoss: 1.0119\n",
      "Train Epoch: 2 [600/3200 (18.75%)]\tLoss: 0.8343\n",
      "Train Epoch: 2 [800/3200 (25.0%)]\tLoss: 0.8150\n",
      "Train Epoch: 2 [1000/3200 (31.25%)]\tLoss: 0.9179\n",
      "Train Epoch: 2 [1200/3200 (37.5%)]\tLoss: 1.2287\n",
      "Train Epoch: 2 [1400/3200 (43.75%)]\tLoss: 0.8248\n",
      "Train Epoch: 2 [1600/3200 (50.0%)]\tLoss: 0.7532\n",
      "Train Epoch: 2 [1800/3200 (56.25%)]\tLoss: 0.8492\n",
      "Train Epoch: 2 [2000/3200 (62.5%)]\tLoss: 1.4015\n",
      "Train Epoch: 2 [2200/3200 (68.75%)]\tLoss: 0.8255\n",
      "Train Epoch: 2 [2400/3200 (75.0%)]\tLoss: 0.9190\n",
      "Train Epoch: 2 [2600/3200 (81.25%)]\tLoss: 0.7533\n",
      "Train Epoch: 2 [2800/3200 (87.5%)]\tLoss: 0.7770\n",
      "Train Epoch: 2 [3000/3200 (93.75%)]\tLoss: 0.7514\n",
      "Train Epoch: 2 [3200/3200 (100.0%)]\tLoss: 1.1372\n",
      "Done epoch #2, time for this epoch: 314.2963197231293s\n",
      "Start epoch #3, learning rate for this epoch: [0.0002]\n",
      "Train Epoch: 3 [200/3200 (6.25%)]\tLoss: 0.6315\n",
      "Train Epoch: 3 [400/3200 (12.5%)]\tLoss: 0.8620\n",
      "Train Epoch: 3 [600/3200 (18.75%)]\tLoss: 0.6310\n",
      "Train Epoch: 3 [800/3200 (25.0%)]\tLoss: 0.8241\n",
      "Train Epoch: 3 [1000/3200 (31.25%)]\tLoss: 0.6604\n",
      "Train Epoch: 3 [1200/3200 (37.5%)]\tLoss: 0.6335\n",
      "Train Epoch: 3 [1400/3200 (43.75%)]\tLoss: 0.6775\n",
      "Train Epoch: 3 [1600/3200 (50.0%)]\tLoss: 0.9701\n",
      "Train Epoch: 3 [1800/3200 (56.25%)]\tLoss: 0.6484\n",
      "Train Epoch: 3 [2000/3200 (62.5%)]\tLoss: 0.6354\n",
      "Train Epoch: 3 [2200/3200 (68.75%)]\tLoss: 0.6809\n",
      "Train Epoch: 3 [2400/3200 (75.0%)]\tLoss: 0.8259\n",
      "Train Epoch: 3 [2600/3200 (81.25%)]\tLoss: 0.6055\n",
      "Train Epoch: 3 [2800/3200 (87.5%)]\tLoss: 0.6733\n",
      "Train Epoch: 3 [3000/3200 (93.75%)]\tLoss: 0.7400\n",
      "Train Epoch: 3 [3200/3200 (100.0%)]\tLoss: 0.6538\n",
      "Done epoch #3, time for this epoch: 312.775269985199s\n",
      "Start epoch #4, learning rate for this epoch: [0.0002]\n",
      "Train Epoch: 4 [200/3200 (6.25%)]\tLoss: 0.7679\n",
      "Train Epoch: 4 [400/3200 (12.5%)]\tLoss: 0.8425\n",
      "Train Epoch: 4 [600/3200 (18.75%)]\tLoss: 0.6596\n",
      "Train Epoch: 4 [800/3200 (25.0%)]\tLoss: 0.6163\n",
      "Train Epoch: 4 [1000/3200 (31.25%)]\tLoss: 0.6217\n",
      "Train Epoch: 4 [1200/3200 (37.5%)]\tLoss: 0.8757\n",
      "Train Epoch: 4 [1400/3200 (43.75%)]\tLoss: 0.7503\n",
      "Train Epoch: 4 [1600/3200 (50.0%)]\tLoss: 0.6337\n",
      "Train Epoch: 4 [1800/3200 (56.25%)]\tLoss: 0.6534\n",
      "Train Epoch: 4 [2000/3200 (62.5%)]\tLoss: 0.6646\n",
      "Train Epoch: 4 [2200/3200 (68.75%)]\tLoss: 0.6325\n",
      "Train Epoch: 4 [2400/3200 (75.0%)]\tLoss: 0.6999\n",
      "Train Epoch: 4 [2600/3200 (81.25%)]\tLoss: 0.6199\n",
      "Train Epoch: 4 [2800/3200 (87.5%)]\tLoss: 0.5453\n",
      "Train Epoch: 4 [3000/3200 (93.75%)]\tLoss: 0.5870\n",
      "Train Epoch: 4 [3200/3200 (100.0%)]\tLoss: 0.6286\n",
      "Done epoch #4, time for this epoch: 314.01560950279236s\n",
      "Start epoch #5, learning rate for this epoch: [0.00012]\n",
      "Train Epoch: 5 [200/3200 (6.25%)]\tLoss: 0.5846\n",
      "Train Epoch: 5 [400/3200 (12.5%)]\tLoss: 0.4719\n",
      "Train Epoch: 5 [600/3200 (18.75%)]\tLoss: 0.5724\n",
      "Train Epoch: 5 [800/3200 (25.0%)]\tLoss: 0.5771\n",
      "Train Epoch: 5 [1000/3200 (31.25%)]\tLoss: 0.5194\n",
      "Train Epoch: 5 [1200/3200 (37.5%)]\tLoss: 0.6169\n",
      "Train Epoch: 5 [1400/3200 (43.75%)]\tLoss: 0.6300\n",
      "Train Epoch: 5 [1600/3200 (50.0%)]\tLoss: 0.5824\n",
      "Train Epoch: 5 [1800/3200 (56.25%)]\tLoss: 0.4612\n",
      "Train Epoch: 5 [2000/3200 (62.5%)]\tLoss: 0.5834\n",
      "Train Epoch: 5 [2200/3200 (68.75%)]\tLoss: 0.5878\n",
      "Train Epoch: 5 [2400/3200 (75.0%)]\tLoss: 0.6943\n",
      "Train Epoch: 5 [2600/3200 (81.25%)]\tLoss: 0.5508\n",
      "Train Epoch: 5 [2800/3200 (87.5%)]\tLoss: 0.6358\n",
      "Train Epoch: 5 [3000/3200 (93.75%)]\tLoss: 0.4793\n",
      "Train Epoch: 5 [3200/3200 (100.0%)]\tLoss: 0.5251\n",
      "Done epoch #5, time for this epoch: 312.9037528038025s\n",
      "Start epoch #6, learning rate for this epoch: [0.00012]\n",
      "Train Epoch: 6 [200/3200 (6.25%)]\tLoss: 0.5189\n",
      "Train Epoch: 6 [400/3200 (12.5%)]\tLoss: 0.5420\n",
      "Train Epoch: 6 [600/3200 (18.75%)]\tLoss: 0.5646\n",
      "Train Epoch: 6 [800/3200 (25.0%)]\tLoss: 0.5549\n",
      "Train Epoch: 6 [1000/3200 (31.25%)]\tLoss: 0.4665\n",
      "Train Epoch: 6 [1200/3200 (37.5%)]\tLoss: 0.5804\n",
      "Train Epoch: 6 [1400/3200 (43.75%)]\tLoss: 0.6040\n",
      "Train Epoch: 6 [1600/3200 (50.0%)]\tLoss: 0.6060\n",
      "Train Epoch: 6 [1800/3200 (56.25%)]\tLoss: 0.5864\n",
      "Train Epoch: 6 [2000/3200 (62.5%)]\tLoss: 3.2795\n",
      "Train Epoch: 6 [2200/3200 (68.75%)]\tLoss: 0.5563\n",
      "Train Epoch: 6 [2400/3200 (75.0%)]\tLoss: 0.5925\n",
      "Train Epoch: 6 [2600/3200 (81.25%)]\tLoss: 0.5638\n",
      "Train Epoch: 6 [2800/3200 (87.5%)]\tLoss: 0.5822\n",
      "Train Epoch: 6 [3000/3200 (93.75%)]\tLoss: 0.6244\n",
      "Train Epoch: 6 [3200/3200 (100.0%)]\tLoss: 0.5825\n",
      "Done epoch #6, time for this epoch: 313.3125820159912s\n",
      "Start epoch #7, learning rate for this epoch: [0.00012]\n",
      "Train Epoch: 7 [200/3200 (6.25%)]\tLoss: 0.5933\n",
      "Train Epoch: 7 [400/3200 (12.5%)]\tLoss: 0.5632\n",
      "Train Epoch: 7 [600/3200 (18.75%)]\tLoss: 0.6882\n",
      "Train Epoch: 7 [800/3200 (25.0%)]\tLoss: 0.6002\n",
      "Train Epoch: 7 [1000/3200 (31.25%)]\tLoss: 0.5287\n",
      "Train Epoch: 7 [1200/3200 (37.5%)]\tLoss: 0.6018\n",
      "Train Epoch: 7 [1400/3200 (43.75%)]\tLoss: 0.4011\n",
      "Train Epoch: 7 [1600/3200 (50.0%)]\tLoss: 0.5899\n",
      "Train Epoch: 7 [1800/3200 (56.25%)]\tLoss: 0.6713\n",
      "Train Epoch: 7 [2000/3200 (62.5%)]\tLoss: 0.5477\n",
      "Train Epoch: 7 [2200/3200 (68.75%)]\tLoss: 0.4792\n",
      "Train Epoch: 7 [2400/3200 (75.0%)]\tLoss: 0.9002\n",
      "Train Epoch: 7 [2600/3200 (81.25%)]\tLoss: 0.5457\n",
      "Train Epoch: 7 [2800/3200 (87.5%)]\tLoss: 0.5795\n",
      "Train Epoch: 7 [3000/3200 (93.75%)]\tLoss: 0.6112\n",
      "Train Epoch: 7 [3200/3200 (100.0%)]\tLoss: 0.5815\n",
      "Done epoch #7, time for this epoch: 314.3557798862457s\n",
      "Start epoch #8, learning rate for this epoch: [0.00012]\n",
      "Train Epoch: 8 [200/3200 (6.25%)]\tLoss: 0.5513\n",
      "Train Epoch: 8 [400/3200 (12.5%)]\tLoss: 0.5726\n",
      "Train Epoch: 8 [600/3200 (18.75%)]\tLoss: 0.5541\n",
      "Train Epoch: 8 [800/3200 (25.0%)]\tLoss: 0.5589\n",
      "Train Epoch: 8 [1000/3200 (31.25%)]\tLoss: 0.6120\n",
      "Train Epoch: 8 [1200/3200 (37.5%)]\tLoss: 0.5754\n",
      "Train Epoch: 8 [1400/3200 (43.75%)]\tLoss: 0.5985\n",
      "Train Epoch: 8 [1600/3200 (50.0%)]\tLoss: 0.4704\n",
      "Train Epoch: 8 [1800/3200 (56.25%)]\tLoss: 0.6800\n",
      "Train Epoch: 8 [2000/3200 (62.5%)]\tLoss: 0.5717\n",
      "Train Epoch: 8 [2200/3200 (68.75%)]\tLoss: 0.5712\n",
      "Train Epoch: 8 [2400/3200 (75.0%)]\tLoss: 0.5131\n",
      "Train Epoch: 8 [2600/3200 (81.25%)]\tLoss: 0.5814\n",
      "Train Epoch: 8 [2800/3200 (87.5%)]\tLoss: 0.5191\n",
      "Train Epoch: 8 [3000/3200 (93.75%)]\tLoss: 0.5827\n",
      "Train Epoch: 8 [3200/3200 (100.0%)]\tLoss: 0.5925\n",
      "Done epoch #8, time for this epoch: 313.2643611431122s\n",
      "Start epoch #9, learning rate for this epoch: [7.2e-05]\n",
      "Train Epoch: 9 [200/3200 (6.25%)]\tLoss: 0.5694\n",
      "Train Epoch: 9 [400/3200 (12.5%)]\tLoss: 0.5854\n",
      "Train Epoch: 9 [600/3200 (18.75%)]\tLoss: 0.5836\n",
      "Train Epoch: 9 [800/3200 (25.0%)]\tLoss: 0.5488\n",
      "Train Epoch: 9 [1000/3200 (31.25%)]\tLoss: 0.4025\n",
      "Train Epoch: 9 [1200/3200 (37.5%)]\tLoss: 0.5839\n",
      "Train Epoch: 9 [1400/3200 (43.75%)]\tLoss: 0.5738\n",
      "Train Epoch: 9 [1600/3200 (50.0%)]\tLoss: 0.5432\n",
      "Train Epoch: 9 [1800/3200 (56.25%)]\tLoss: 0.4935\n",
      "Train Epoch: 9 [2000/3200 (62.5%)]\tLoss: 0.5447\n",
      "Train Epoch: 9 [2200/3200 (68.75%)]\tLoss: 0.4788\n",
      "Train Epoch: 9 [2400/3200 (75.0%)]\tLoss: 0.5701\n",
      "Train Epoch: 9 [2600/3200 (81.25%)]\tLoss: 0.3268\n",
      "Train Epoch: 9 [2800/3200 (87.5%)]\tLoss: 0.5397\n",
      "Train Epoch: 9 [3000/3200 (93.75%)]\tLoss: 0.4567\n",
      "Train Epoch: 9 [3200/3200 (100.0%)]\tLoss: 0.5831\n",
      "Done epoch #9, time for this epoch: 315.88082218170166s\n",
      "Start epoch #10, learning rate for this epoch: [7.2e-05]\n",
      "Train Epoch: 10 [200/3200 (6.25%)]\tLoss: 0.6850\n",
      "Train Epoch: 10 [400/3200 (12.5%)]\tLoss: 0.7334\n",
      "Train Epoch: 10 [600/3200 (18.75%)]\tLoss: 0.4245\n",
      "Train Epoch: 10 [800/3200 (25.0%)]\tLoss: 0.4272\n",
      "Train Epoch: 10 [1000/3200 (31.25%)]\tLoss: 0.4302\n",
      "Train Epoch: 10 [1200/3200 (37.5%)]\tLoss: 0.4347\n",
      "Train Epoch: 10 [1400/3200 (43.75%)]\tLoss: 0.5511\n",
      "Train Epoch: 10 [1600/3200 (50.0%)]\tLoss: 0.5443\n",
      "Train Epoch: 10 [1800/3200 (56.25%)]\tLoss: 0.5921\n",
      "Train Epoch: 10 [2000/3200 (62.5%)]\tLoss: 0.3621\n",
      "Train Epoch: 10 [2200/3200 (68.75%)]\tLoss: 0.5490\n",
      "Train Epoch: 10 [2400/3200 (75.0%)]\tLoss: 0.5381\n",
      "Train Epoch: 10 [2600/3200 (81.25%)]\tLoss: 0.5294\n",
      "Train Epoch: 10 [2800/3200 (87.5%)]\tLoss: 0.5162\n",
      "Train Epoch: 10 [3000/3200 (93.75%)]\tLoss: 0.5890\n",
      "Train Epoch: 10 [3200/3200 (100.0%)]\tLoss: 0.4721\n",
      "Done epoch #10, time for this epoch: 313.9940688610077s\n",
      "Start epoch #11, learning rate for this epoch: [7.2e-05]\n",
      "Train Epoch: 11 [200/3200 (6.25%)]\tLoss: 0.5741\n",
      "Train Epoch: 11 [400/3200 (12.5%)]\tLoss: 0.5032\n",
      "Train Epoch: 11 [600/3200 (18.75%)]\tLoss: 0.5463\n",
      "Train Epoch: 11 [800/3200 (25.0%)]\tLoss: 0.5742\n",
      "Train Epoch: 11 [1000/3200 (31.25%)]\tLoss: 0.5725\n",
      "Train Epoch: 11 [1200/3200 (37.5%)]\tLoss: 0.5811\n",
      "Train Epoch: 11 [1400/3200 (43.75%)]\tLoss: 0.5300\n",
      "Train Epoch: 11 [1600/3200 (50.0%)]\tLoss: 0.5277\n",
      "Train Epoch: 11 [1800/3200 (56.25%)]\tLoss: 0.5077\n",
      "Train Epoch: 11 [2000/3200 (62.5%)]\tLoss: 0.4463\n",
      "Train Epoch: 11 [2200/3200 (68.75%)]\tLoss: 0.5806\n",
      "Train Epoch: 11 [2400/3200 (75.0%)]\tLoss: 0.5342\n",
      "Train Epoch: 11 [2600/3200 (81.25%)]\tLoss: 0.4990\n",
      "Train Epoch: 11 [2800/3200 (87.5%)]\tLoss: 0.5729\n",
      "Train Epoch: 11 [3000/3200 (93.75%)]\tLoss: 0.5318\n",
      "Train Epoch: 11 [3200/3200 (100.0%)]\tLoss: 0.5903\n",
      "Done epoch #11, time for this epoch: 312.3730399608612s\n",
      "Start epoch #12, learning rate for this epoch: [7.2e-05]\n",
      "Train Epoch: 12 [200/3200 (6.25%)]\tLoss: 0.4634\n",
      "Train Epoch: 12 [400/3200 (12.5%)]\tLoss: 0.5332\n",
      "Train Epoch: 12 [600/3200 (18.75%)]\tLoss: 0.5111\n",
      "Train Epoch: 12 [800/3200 (25.0%)]\tLoss: 0.4417\n",
      "Train Epoch: 12 [1000/3200 (31.25%)]\tLoss: 0.5376\n",
      "Train Epoch: 12 [1200/3200 (37.5%)]\tLoss: 0.5730\n",
      "Train Epoch: 12 [1400/3200 (43.75%)]\tLoss: 0.4461\n",
      "Train Epoch: 12 [1600/3200 (50.0%)]\tLoss: 0.5845\n",
      "Train Epoch: 12 [1800/3200 (56.25%)]\tLoss: 0.5318\n",
      "Train Epoch: 12 [2000/3200 (62.5%)]\tLoss: 0.5225\n",
      "Train Epoch: 12 [2200/3200 (68.75%)]\tLoss: 0.4434\n",
      "Train Epoch: 12 [2400/3200 (75.0%)]\tLoss: 0.5494\n",
      "Train Epoch: 12 [2600/3200 (81.25%)]\tLoss: 0.5271\n",
      "Train Epoch: 12 [2800/3200 (87.5%)]\tLoss: 0.5969\n",
      "Train Epoch: 12 [3000/3200 (93.75%)]\tLoss: 0.5461\n",
      "Train Epoch: 12 [3200/3200 (100.0%)]\tLoss: 0.5696\n",
      "Done epoch #12, time for this epoch: 312.604430437088s\n",
      "Start epoch #13, learning rate for this epoch: [4.32e-05]\n",
      "Train Epoch: 13 [200/3200 (6.25%)]\tLoss: 0.5909\n",
      "Train Epoch: 13 [400/3200 (12.5%)]\tLoss: 0.4976\n",
      "Train Epoch: 13 [600/3200 (18.75%)]\tLoss: 0.5468\n",
      "Train Epoch: 13 [800/3200 (25.0%)]\tLoss: 0.5432\n",
      "Train Epoch: 13 [1000/3200 (31.25%)]\tLoss: 0.5607\n",
      "Train Epoch: 13 [1200/3200 (37.5%)]\tLoss: 0.5384\n",
      "Train Epoch: 13 [1400/3200 (43.75%)]\tLoss: 0.5408\n",
      "Train Epoch: 13 [1600/3200 (50.0%)]\tLoss: 0.5416\n",
      "Train Epoch: 13 [1800/3200 (56.25%)]\tLoss: 0.5718\n",
      "Train Epoch: 13 [2000/3200 (62.5%)]\tLoss: 0.5474\n",
      "Train Epoch: 13 [2200/3200 (68.75%)]\tLoss: 0.5793\n",
      "Train Epoch: 13 [2400/3200 (75.0%)]\tLoss: 0.4999\n",
      "Train Epoch: 13 [2600/3200 (81.25%)]\tLoss: 0.5037\n",
      "Train Epoch: 13 [2800/3200 (87.5%)]\tLoss: 0.5116\n",
      "Train Epoch: 13 [3000/3200 (93.75%)]\tLoss: 0.5709\n",
      "Train Epoch: 13 [3200/3200 (100.0%)]\tLoss: 0.5022\n",
      "Done epoch #13, time for this epoch: 315.02539348602295s\n",
      "Start epoch #14, learning rate for this epoch: [4.32e-05]\n",
      "Train Epoch: 14 [200/3200 (6.25%)]\tLoss: 0.5385\n",
      "Train Epoch: 14 [400/3200 (12.5%)]\tLoss: 0.5744\n",
      "Train Epoch: 14 [600/3200 (18.75%)]\tLoss: 0.4197\n",
      "Train Epoch: 14 [800/3200 (25.0%)]\tLoss: 0.4131\n",
      "Train Epoch: 14 [1000/3200 (31.25%)]\tLoss: 0.4447\n",
      "Train Epoch: 14 [1200/3200 (37.5%)]\tLoss: 0.4933\n",
      "Train Epoch: 14 [1400/3200 (43.75%)]\tLoss: 0.5367\n",
      "Train Epoch: 14 [1600/3200 (50.0%)]\tLoss: 0.5454\n",
      "Train Epoch: 14 [1800/3200 (56.25%)]\tLoss: 0.5741\n",
      "Train Epoch: 14 [2000/3200 (62.5%)]\tLoss: 0.4932\n",
      "Train Epoch: 14 [2200/3200 (68.75%)]\tLoss: 0.4418\n",
      "Train Epoch: 14 [2400/3200 (75.0%)]\tLoss: 0.5383\n",
      "Train Epoch: 14 [2600/3200 (81.25%)]\tLoss: 0.5719\n",
      "Train Epoch: 14 [2800/3200 (87.5%)]\tLoss: 0.4813\n",
      "Train Epoch: 14 [3000/3200 (93.75%)]\tLoss: 0.5076\n",
      "Train Epoch: 14 [3200/3200 (100.0%)]\tLoss: 0.5014\n",
      "Done epoch #14, time for this epoch: 312.9668731689453s\n",
      "Start epoch #15, learning rate for this epoch: [4.32e-05]\n",
      "Train Epoch: 15 [200/3200 (6.25%)]\tLoss: 0.4749\n",
      "Train Epoch: 15 [400/3200 (12.5%)]\tLoss: 0.4648\n",
      "Train Epoch: 15 [600/3200 (18.75%)]\tLoss: 0.5101\n",
      "Train Epoch: 15 [800/3200 (25.0%)]\tLoss: 0.5392\n",
      "Train Epoch: 15 [1000/3200 (31.25%)]\tLoss: 0.4289\n",
      "Train Epoch: 15 [1200/3200 (37.5%)]\tLoss: 0.4684\n",
      "Train Epoch: 15 [1400/3200 (43.75%)]\tLoss: 0.4151\n",
      "Train Epoch: 15 [1600/3200 (50.0%)]\tLoss: 0.5150\n",
      "Train Epoch: 15 [1800/3200 (56.25%)]\tLoss: 0.5350\n",
      "Train Epoch: 15 [2000/3200 (62.5%)]\tLoss: 0.4095\n",
      "Train Epoch: 15 [2200/3200 (68.75%)]\tLoss: 0.9826\n",
      "Train Epoch: 15 [2400/3200 (75.0%)]\tLoss: 0.5028\n",
      "Train Epoch: 15 [2600/3200 (81.25%)]\tLoss: 0.5375\n",
      "Train Epoch: 15 [2800/3200 (87.5%)]\tLoss: 0.5667\n",
      "Train Epoch: 15 [3000/3200 (93.75%)]\tLoss: 0.5369\n",
      "Train Epoch: 15 [3200/3200 (100.0%)]\tLoss: 0.5709\n",
      "Done epoch #15, time for this epoch: 312.4472165107727s\n",
      "Start epoch #16, learning rate for this epoch: [4.32e-05]\n",
      "Train Epoch: 16 [200/3200 (6.25%)]\tLoss: 0.5578\n",
      "Train Epoch: 16 [400/3200 (12.5%)]\tLoss: 0.5713\n",
      "Train Epoch: 16 [600/3200 (18.75%)]\tLoss: 0.5326\n",
      "Train Epoch: 16 [800/3200 (25.0%)]\tLoss: 0.5261\n",
      "Train Epoch: 16 [1000/3200 (31.25%)]\tLoss: 0.5723\n",
      "Train Epoch: 16 [1200/3200 (37.5%)]\tLoss: 0.5665\n",
      "Train Epoch: 16 [1400/3200 (43.75%)]\tLoss: 0.4942\n",
      "Train Epoch: 16 [1600/3200 (50.0%)]\tLoss: 0.5283\n",
      "Train Epoch: 16 [1800/3200 (56.25%)]\tLoss: 0.4282\n",
      "Train Epoch: 16 [2000/3200 (62.5%)]\tLoss: 0.5664\n",
      "Train Epoch: 16 [2200/3200 (68.75%)]\tLoss: 0.5163\n",
      "Train Epoch: 16 [2400/3200 (75.0%)]\tLoss: 0.4540\n",
      "Train Epoch: 16 [2600/3200 (81.25%)]\tLoss: 0.5140\n",
      "Train Epoch: 16 [2800/3200 (87.5%)]\tLoss: 0.5674\n",
      "Train Epoch: 16 [3000/3200 (93.75%)]\tLoss: 0.5378\n",
      "Train Epoch: 16 [3200/3200 (100.0%)]\tLoss: 0.5200\n",
      "Done epoch #16, time for this epoch: 313.816775560379s\n",
      "Start epoch #17, learning rate for this epoch: [2.592e-05]\n",
      "Train Epoch: 17 [200/3200 (6.25%)]\tLoss: 0.5662\n",
      "Train Epoch: 17 [400/3200 (12.5%)]\tLoss: 0.4441\n",
      "Train Epoch: 17 [600/3200 (18.75%)]\tLoss: 0.4908\n",
      "Train Epoch: 17 [800/3200 (25.0%)]\tLoss: 0.5692\n",
      "Train Epoch: 17 [1000/3200 (31.25%)]\tLoss: 0.5141\n",
      "Train Epoch: 17 [1200/3200 (37.5%)]\tLoss: 0.5404\n",
      "Train Epoch: 17 [1400/3200 (43.75%)]\tLoss: 0.4279\n",
      "Train Epoch: 17 [1600/3200 (50.0%)]\tLoss: 0.5474\n",
      "Train Epoch: 17 [1800/3200 (56.25%)]\tLoss: 0.5364\n",
      "Train Epoch: 17 [2000/3200 (62.5%)]\tLoss: 0.5321\n",
      "Train Epoch: 17 [2200/3200 (68.75%)]\tLoss: 0.5299\n",
      "Train Epoch: 17 [2400/3200 (75.0%)]\tLoss: 0.5328\n",
      "Train Epoch: 17 [2600/3200 (81.25%)]\tLoss: 0.4322\n",
      "Train Epoch: 17 [2800/3200 (87.5%)]\tLoss: 0.4720\n",
      "Train Epoch: 17 [3000/3200 (93.75%)]\tLoss: 0.5690\n",
      "Train Epoch: 17 [3200/3200 (100.0%)]\tLoss: 0.3416\n",
      "Done epoch #17, time for this epoch: 313.9543077945709s\n",
      "Start epoch #18, learning rate for this epoch: [2.592e-05]\n",
      "Train Epoch: 18 [200/3200 (6.25%)]\tLoss: 0.5337\n",
      "Train Epoch: 18 [400/3200 (12.5%)]\tLoss: 0.5288\n",
      "Train Epoch: 18 [600/3200 (18.75%)]\tLoss: 0.4017\n",
      "Train Epoch: 18 [800/3200 (25.0%)]\tLoss: 0.5453\n",
      "Train Epoch: 18 [1000/3200 (31.25%)]\tLoss: 0.5691\n",
      "Train Epoch: 18 [1200/3200 (37.5%)]\tLoss: 0.4378\n",
      "Train Epoch: 18 [1400/3200 (43.75%)]\tLoss: 0.5010\n",
      "Train Epoch: 18 [1600/3200 (50.0%)]\tLoss: 0.5672\n",
      "Train Epoch: 18 [1800/3200 (56.25%)]\tLoss: 0.5267\n",
      "Train Epoch: 18 [2000/3200 (62.5%)]\tLoss: 0.5414\n",
      "Train Epoch: 18 [2200/3200 (68.75%)]\tLoss: 0.5262\n",
      "Train Epoch: 18 [2400/3200 (75.0%)]\tLoss: 0.4999\n",
      "Train Epoch: 18 [2600/3200 (81.25%)]\tLoss: 0.5347\n",
      "Train Epoch: 18 [2800/3200 (87.5%)]\tLoss: 0.5387\n",
      "Train Epoch: 18 [3000/3200 (93.75%)]\tLoss: 0.4458\n",
      "Train Epoch: 18 [3200/3200 (100.0%)]\tLoss: 0.4134\n",
      "Done epoch #18, time for this epoch: 312.5855133533478s\n",
      "Start epoch #19, learning rate for this epoch: [2.592e-05]\n",
      "Train Epoch: 19 [200/3200 (6.25%)]\tLoss: 0.5631\n",
      "Train Epoch: 19 [400/3200 (12.5%)]\tLoss: 0.4424\n",
      "Train Epoch: 19 [600/3200 (18.75%)]\tLoss: 0.5382\n",
      "Train Epoch: 19 [800/3200 (25.0%)]\tLoss: 0.4456\n",
      "Train Epoch: 19 [1000/3200 (31.25%)]\tLoss: 0.5675\n",
      "Train Epoch: 19 [1200/3200 (37.5%)]\tLoss: 0.5682\n",
      "Train Epoch: 19 [1400/3200 (43.75%)]\tLoss: 0.5310\n",
      "Train Epoch: 19 [1600/3200 (50.0%)]\tLoss: 0.5366\n",
      "Train Epoch: 19 [1800/3200 (56.25%)]\tLoss: 0.5355\n",
      "Train Epoch: 19 [2000/3200 (62.5%)]\tLoss: 0.5043\n",
      "Train Epoch: 19 [2200/3200 (68.75%)]\tLoss: 0.5720\n",
      "Train Epoch: 19 [2400/3200 (75.0%)]\tLoss: 0.4203\n",
      "Train Epoch: 19 [2600/3200 (81.25%)]\tLoss: 0.5610\n",
      "Train Epoch: 19 [2800/3200 (87.5%)]\tLoss: 0.5039\n",
      "Train Epoch: 19 [3000/3200 (93.75%)]\tLoss: 0.2933\n",
      "Train Epoch: 19 [3200/3200 (100.0%)]\tLoss: 0.5309\n",
      "Done epoch #19, time for this epoch: 312.70795369148254s\n",
      "Start epoch #20, learning rate for this epoch: [2.592e-05]\n",
      "Train Epoch: 20 [200/3200 (6.25%)]\tLoss: 0.5288\n",
      "Train Epoch: 20 [400/3200 (12.5%)]\tLoss: 0.5398\n",
      "Train Epoch: 20 [600/3200 (18.75%)]\tLoss: 0.4415\n",
      "Train Epoch: 20 [800/3200 (25.0%)]\tLoss: 0.4930\n",
      "Train Epoch: 20 [1000/3200 (31.25%)]\tLoss: 0.4515\n",
      "Train Epoch: 20 [1200/3200 (37.5%)]\tLoss: 0.4881\n",
      "Train Epoch: 20 [1400/3200 (43.75%)]\tLoss: 0.5524\n",
      "Train Epoch: 20 [1600/3200 (50.0%)]\tLoss: 0.4110\n",
      "Train Epoch: 20 [1800/3200 (56.25%)]\tLoss: 0.5722\n",
      "Train Epoch: 20 [2000/3200 (62.5%)]\tLoss: 0.5278\n",
      "Train Epoch: 20 [2200/3200 (68.75%)]\tLoss: 0.5643\n",
      "Train Epoch: 20 [2400/3200 (75.0%)]\tLoss: 0.4672\n",
      "Train Epoch: 20 [2600/3200 (81.25%)]\tLoss: 0.5026\n",
      "Train Epoch: 20 [2800/3200 (87.5%)]\tLoss: 0.5314\n",
      "Train Epoch: 20 [3000/3200 (93.75%)]\tLoss: 0.5690\n",
      "Train Epoch: 20 [3200/3200 (100.0%)]\tLoss: 0.4050\n",
      "Done epoch #20, time for this epoch: 313.838773727417s\n",
      "Start epoch #21, learning rate for this epoch: [1.5552e-05]\n",
      "Train Epoch: 21 [200/3200 (6.25%)]\tLoss: 0.4066\n",
      "Train Epoch: 21 [400/3200 (12.5%)]\tLoss: 0.4008\n",
      "Train Epoch: 21 [600/3200 (18.75%)]\tLoss: 0.4926\n",
      "Train Epoch: 21 [800/3200 (25.0%)]\tLoss: 0.4945\n",
      "Train Epoch: 21 [1000/3200 (31.25%)]\tLoss: 0.5395\n",
      "Train Epoch: 21 [1200/3200 (37.5%)]\tLoss: 0.5671\n",
      "Train Epoch: 21 [1400/3200 (43.75%)]\tLoss: 0.5045\n",
      "Train Epoch: 21 [1600/3200 (50.0%)]\tLoss: 0.5430\n",
      "Train Epoch: 21 [1800/3200 (56.25%)]\tLoss: 0.4447\n",
      "Train Epoch: 21 [2000/3200 (62.5%)]\tLoss: 0.4143\n",
      "Train Epoch: 21 [2200/3200 (68.75%)]\tLoss: 0.5045\n",
      "Train Epoch: 21 [2400/3200 (75.0%)]\tLoss: 0.4577\n",
      "Train Epoch: 21 [2600/3200 (81.25%)]\tLoss: 0.5304\n",
      "Train Epoch: 21 [2800/3200 (87.5%)]\tLoss: 0.5006\n",
      "Train Epoch: 21 [3000/3200 (93.75%)]\tLoss: 0.4313\n",
      "Train Epoch: 21 [3200/3200 (100.0%)]\tLoss: 0.5267\n",
      "Done epoch #21, time for this epoch: 315.3918471336365s\n",
      "Start epoch #22, learning rate for this epoch: [1.5552e-05]\n",
      "Train Epoch: 22 [200/3200 (6.25%)]\tLoss: 0.5415\n",
      "Train Epoch: 22 [400/3200 (12.5%)]\tLoss: 0.5633\n",
      "Train Epoch: 22 [600/3200 (18.75%)]\tLoss: 0.5724\n",
      "Train Epoch: 22 [800/3200 (25.0%)]\tLoss: 0.5675\n",
      "Train Epoch: 22 [1000/3200 (31.25%)]\tLoss: 0.5656\n",
      "Train Epoch: 22 [1200/3200 (37.5%)]\tLoss: 0.5690\n",
      "Train Epoch: 22 [1400/3200 (43.75%)]\tLoss: 0.4239\n",
      "Train Epoch: 22 [1600/3200 (50.0%)]\tLoss: 0.4955\n",
      "Train Epoch: 22 [1800/3200 (56.25%)]\tLoss: 0.4434\n",
      "Train Epoch: 22 [2000/3200 (62.5%)]\tLoss: 0.5100\n",
      "Train Epoch: 22 [2200/3200 (68.75%)]\tLoss: 0.4352\n",
      "Train Epoch: 22 [2400/3200 (75.0%)]\tLoss: 0.5269\n",
      "Train Epoch: 22 [2600/3200 (81.25%)]\tLoss: 0.5322\n",
      "Train Epoch: 22 [2800/3200 (87.5%)]\tLoss: 0.4696\n",
      "Train Epoch: 22 [3000/3200 (93.75%)]\tLoss: 0.4644\n",
      "Train Epoch: 22 [3200/3200 (100.0%)]\tLoss: 0.5271\n",
      "Done epoch #22, time for this epoch: 314.94875836372375s\n",
      "Start epoch #23, learning rate for this epoch: [1.5552e-05]\n",
      "Train Epoch: 23 [200/3200 (6.25%)]\tLoss: 0.5263\n",
      "Train Epoch: 23 [400/3200 (12.5%)]\tLoss: 0.4996\n",
      "Train Epoch: 23 [600/3200 (18.75%)]\tLoss: 0.5984\n",
      "Train Epoch: 23 [800/3200 (25.0%)]\tLoss: 0.5600\n",
      "Train Epoch: 23 [1000/3200 (31.25%)]\tLoss: 0.5480\n",
      "Train Epoch: 23 [1200/3200 (37.5%)]\tLoss: 0.5670\n",
      "Train Epoch: 23 [1400/3200 (43.75%)]\tLoss: 0.5293\n",
      "Train Epoch: 23 [1600/3200 (50.0%)]\tLoss: 0.5703\n",
      "Train Epoch: 23 [1800/3200 (56.25%)]\tLoss: 0.4938\n",
      "Train Epoch: 23 [2000/3200 (62.5%)]\tLoss: 0.5710\n",
      "Train Epoch: 23 [2200/3200 (68.75%)]\tLoss: 0.4231\n",
      "Train Epoch: 23 [2400/3200 (75.0%)]\tLoss: 0.4986\n",
      "Train Epoch: 23 [2600/3200 (81.25%)]\tLoss: 0.4395\n",
      "Train Epoch: 23 [2800/3200 (87.5%)]\tLoss: 0.5741\n",
      "Train Epoch: 23 [3000/3200 (93.75%)]\tLoss: 0.5406\n",
      "Train Epoch: 23 [3200/3200 (100.0%)]\tLoss: 0.4084\n",
      "Done epoch #23, time for this epoch: 313.2384743690491s\n",
      "Start epoch #24, learning rate for this epoch: [1.5552e-05]\n",
      "Train Epoch: 24 [200/3200 (6.25%)]\tLoss: 0.5611\n",
      "Train Epoch: 24 [400/3200 (12.5%)]\tLoss: 0.5303\n",
      "Train Epoch: 24 [600/3200 (18.75%)]\tLoss: 0.4347\n",
      "Train Epoch: 24 [800/3200 (25.0%)]\tLoss: 0.5606\n",
      "Train Epoch: 24 [1000/3200 (31.25%)]\tLoss: 0.5349\n",
      "Train Epoch: 24 [1200/3200 (37.5%)]\tLoss: 0.5007\n",
      "Train Epoch: 24 [1400/3200 (43.75%)]\tLoss: 0.5685\n",
      "Train Epoch: 24 [1600/3200 (50.0%)]\tLoss: 0.5267\n",
      "Train Epoch: 24 [1800/3200 (56.25%)]\tLoss: 0.5313\n",
      "Train Epoch: 24 [2000/3200 (62.5%)]\tLoss: 0.5328\n",
      "Train Epoch: 24 [2200/3200 (68.75%)]\tLoss: 0.4990\n",
      "Train Epoch: 24 [2400/3200 (75.0%)]\tLoss: 0.4979\n",
      "Train Epoch: 24 [2600/3200 (81.25%)]\tLoss: 0.4962\n",
      "Train Epoch: 24 [2800/3200 (87.5%)]\tLoss: 0.4919\n",
      "Train Epoch: 24 [3000/3200 (93.75%)]\tLoss: 0.5340\n",
      "Train Epoch: 24 [3200/3200 (100.0%)]\tLoss: 0.5702\n",
      "Done epoch #24, time for this epoch: 315.3196039199829s\n",
      "Start epoch #25, learning rate for this epoch: [9.3312e-06]\n",
      "Train Epoch: 25 [200/3200 (6.25%)]\tLoss: 0.5350\n",
      "Train Epoch: 25 [400/3200 (12.5%)]\tLoss: 0.5361\n",
      "Train Epoch: 25 [600/3200 (18.75%)]\tLoss: 0.3754\n",
      "Train Epoch: 25 [800/3200 (25.0%)]\tLoss: 0.4999\n",
      "Train Epoch: 25 [1000/3200 (31.25%)]\tLoss: 0.4952\n",
      "Train Epoch: 25 [1200/3200 (37.5%)]\tLoss: 0.4706\n",
      "Train Epoch: 25 [1400/3200 (43.75%)]\tLoss: 0.4919\n",
      "Train Epoch: 25 [1600/3200 (50.0%)]\tLoss: 0.5284\n",
      "Train Epoch: 25 [1800/3200 (56.25%)]\tLoss: 0.5290\n",
      "Train Epoch: 25 [2000/3200 (62.5%)]\tLoss: 0.5628\n",
      "Train Epoch: 25 [2200/3200 (68.75%)]\tLoss: 0.5050\n",
      "Train Epoch: 25 [2400/3200 (75.0%)]\tLoss: 0.5338\n",
      "Train Epoch: 25 [2600/3200 (81.25%)]\tLoss: 0.4113\n",
      "Train Epoch: 25 [2800/3200 (87.5%)]\tLoss: 0.4918\n",
      "Train Epoch: 25 [3000/3200 (93.75%)]\tLoss: 0.4419\n",
      "Train Epoch: 25 [3200/3200 (100.0%)]\tLoss: 0.5673\n",
      "Done epoch #25, time for this epoch: 313.09221601486206s\n",
      "Start epoch #26, learning rate for this epoch: [9.3312e-06]\n",
      "Train Epoch: 26 [200/3200 (6.25%)]\tLoss: 0.5626\n",
      "Train Epoch: 26 [400/3200 (12.5%)]\tLoss: 0.5594\n",
      "Train Epoch: 26 [600/3200 (18.75%)]\tLoss: 0.3992\n",
      "Train Epoch: 26 [800/3200 (25.0%)]\tLoss: 0.4923\n",
      "Train Epoch: 26 [1000/3200 (31.25%)]\tLoss: 0.4648\n",
      "Train Epoch: 26 [1200/3200 (37.5%)]\tLoss: 0.5630\n",
      "Train Epoch: 26 [1400/3200 (43.75%)]\tLoss: 0.5671\n",
      "Train Epoch: 26 [1600/3200 (50.0%)]\tLoss: 0.5882\n",
      "Train Epoch: 26 [1800/3200 (56.25%)]\tLoss: 0.5630\n",
      "Train Epoch: 26 [2000/3200 (62.5%)]\tLoss: 0.5693\n",
      "Train Epoch: 26 [2200/3200 (68.75%)]\tLoss: 0.5261\n",
      "Train Epoch: 26 [2400/3200 (75.0%)]\tLoss: 0.5632\n",
      "Train Epoch: 26 [2600/3200 (81.25%)]\tLoss: 0.4356\n",
      "Train Epoch: 26 [2800/3200 (87.5%)]\tLoss: 0.4958\n",
      "Train Epoch: 26 [3000/3200 (93.75%)]\tLoss: 0.5267\n",
      "Train Epoch: 26 [3200/3200 (100.0%)]\tLoss: 0.4557\n",
      "Done epoch #26, time for this epoch: 313.54785442352295s\n",
      "Start epoch #27, learning rate for this epoch: [9.3312e-06]\n",
      "Train Epoch: 27 [200/3200 (6.25%)]\tLoss: 0.5624\n",
      "Train Epoch: 27 [400/3200 (12.5%)]\tLoss: 0.4907\n",
      "Train Epoch: 27 [600/3200 (18.75%)]\tLoss: 0.5679\n",
      "Train Epoch: 27 [800/3200 (25.0%)]\tLoss: 0.4029\n",
      "Train Epoch: 27 [1000/3200 (31.25%)]\tLoss: 0.5676\n",
      "Train Epoch: 27 [1200/3200 (37.5%)]\tLoss: 0.5295\n",
      "Train Epoch: 27 [1400/3200 (43.75%)]\tLoss: 0.4908\n",
      "Train Epoch: 27 [1600/3200 (50.0%)]\tLoss: 0.4576\n",
      "Train Epoch: 27 [1800/3200 (56.25%)]\tLoss: 0.5263\n",
      "Train Epoch: 27 [2000/3200 (62.5%)]\tLoss: 0.5648\n",
      "Train Epoch: 27 [2200/3200 (68.75%)]\tLoss: 0.4057\n",
      "Train Epoch: 27 [2400/3200 (75.0%)]\tLoss: 0.5655\n",
      "Train Epoch: 27 [2600/3200 (81.25%)]\tLoss: 0.5460\n",
      "Train Epoch: 27 [2800/3200 (87.5%)]\tLoss: 0.5282\n",
      "Train Epoch: 27 [3000/3200 (93.75%)]\tLoss: 0.5279\n",
      "Train Epoch: 27 [3200/3200 (100.0%)]\tLoss: 0.5269\n",
      "Done epoch #27, time for this epoch: 311.8590099811554s\n",
      "Start epoch #28, learning rate for this epoch: [9.3312e-06]\n",
      "Train Epoch: 28 [200/3200 (6.25%)]\tLoss: 0.4604\n",
      "Train Epoch: 28 [400/3200 (12.5%)]\tLoss: 0.5642\n",
      "Train Epoch: 28 [600/3200 (18.75%)]\tLoss: 0.4904\n",
      "Train Epoch: 28 [800/3200 (25.0%)]\tLoss: 0.4894\n",
      "Train Epoch: 28 [1000/3200 (31.25%)]\tLoss: 0.5614\n",
      "Train Epoch: 28 [1200/3200 (37.5%)]\tLoss: 0.5052\n",
      "Train Epoch: 28 [1400/3200 (43.75%)]\tLoss: 0.3697\n",
      "Train Epoch: 28 [1600/3200 (50.0%)]\tLoss: 0.5279\n",
      "Train Epoch: 28 [1800/3200 (56.25%)]\tLoss: 0.4953\n",
      "Train Epoch: 28 [2000/3200 (62.5%)]\tLoss: 0.4913\n",
      "Train Epoch: 28 [2200/3200 (68.75%)]\tLoss: 0.4964\n",
      "Train Epoch: 28 [2400/3200 (75.0%)]\tLoss: 0.4988\n",
      "Train Epoch: 28 [2600/3200 (81.25%)]\tLoss: 0.5283\n",
      "Train Epoch: 28 [2800/3200 (87.5%)]\tLoss: 0.4460\n",
      "Train Epoch: 28 [3000/3200 (93.75%)]\tLoss: 0.5697\n",
      "Train Epoch: 28 [3200/3200 (100.0%)]\tLoss: 0.5256\n",
      "Done epoch #28, time for this epoch: 313.66692662239075s\n",
      "Start epoch #29, learning rate for this epoch: [5.59872e-06]\n",
      "Train Epoch: 29 [200/3200 (6.25%)]\tLoss: 0.5283\n",
      "Train Epoch: 29 [400/3200 (12.5%)]\tLoss: 0.4400\n",
      "Train Epoch: 29 [600/3200 (18.75%)]\tLoss: 0.5634\n",
      "Train Epoch: 29 [800/3200 (25.0%)]\tLoss: 0.4531\n",
      "Train Epoch: 29 [1000/3200 (31.25%)]\tLoss: 0.5280\n",
      "Train Epoch: 29 [1200/3200 (37.5%)]\tLoss: 0.5248\n",
      "Train Epoch: 29 [1400/3200 (43.75%)]\tLoss: 0.4996\n",
      "Train Epoch: 29 [1600/3200 (50.0%)]\tLoss: 0.5627\n",
      "Train Epoch: 29 [1800/3200 (56.25%)]\tLoss: 0.4045\n",
      "Train Epoch: 29 [2000/3200 (62.5%)]\tLoss: 0.5008\n",
      "Train Epoch: 29 [2200/3200 (68.75%)]\tLoss: 0.4594\n",
      "Train Epoch: 29 [2400/3200 (75.0%)]\tLoss: 0.4953\n",
      "Train Epoch: 29 [2600/3200 (81.25%)]\tLoss: 0.5389\n",
      "Train Epoch: 29 [2800/3200 (87.5%)]\tLoss: 0.4568\n",
      "Train Epoch: 29 [3000/3200 (93.75%)]\tLoss: 0.4991\n",
      "Train Epoch: 29 [3200/3200 (100.0%)]\tLoss: 0.5640\n",
      "Done epoch #29, time for this epoch: 312.0118827819824s\n",
      "Start epoch #30, learning rate for this epoch: [5.59872e-06]\n",
      "Train Epoch: 30 [200/3200 (6.25%)]\tLoss: 0.4939\n",
      "Train Epoch: 30 [400/3200 (12.5%)]\tLoss: 0.4723\n",
      "Train Epoch: 30 [600/3200 (18.75%)]\tLoss: 0.4890\n",
      "Train Epoch: 30 [800/3200 (25.0%)]\tLoss: 0.3994\n",
      "Train Epoch: 30 [1000/3200 (31.25%)]\tLoss: 0.5055\n",
      "Train Epoch: 30 [1200/3200 (37.5%)]\tLoss: 0.5306\n",
      "Train Epoch: 30 [1400/3200 (43.75%)]\tLoss: 0.4960\n",
      "Train Epoch: 30 [1600/3200 (50.0%)]\tLoss: 0.5377\n",
      "Train Epoch: 30 [1800/3200 (56.25%)]\tLoss: 0.5418\n",
      "Train Epoch: 30 [2000/3200 (62.5%)]\tLoss: 0.4432\n",
      "Train Epoch: 30 [2200/3200 (68.75%)]\tLoss: 0.5264\n",
      "Train Epoch: 30 [2400/3200 (75.0%)]\tLoss: 0.4890\n",
      "Train Epoch: 30 [2600/3200 (81.25%)]\tLoss: 0.5254\n",
      "Train Epoch: 30 [2800/3200 (87.5%)]\tLoss: 0.5285\n",
      "Train Epoch: 30 [3000/3200 (93.75%)]\tLoss: 0.5399\n",
      "Train Epoch: 30 [3200/3200 (100.0%)]\tLoss: 0.5619\n",
      "Done epoch #30, time for this epoch: 313.4391646385193s\n",
      "Start epoch #31, learning rate for this epoch: [5.59872e-06]\n",
      "Train Epoch: 31 [200/3200 (6.25%)]\tLoss: 0.5289\n",
      "Train Epoch: 31 [400/3200 (12.5%)]\tLoss: 0.5636\n",
      "Train Epoch: 31 [600/3200 (18.75%)]\tLoss: 0.5257\n",
      "Train Epoch: 31 [800/3200 (25.0%)]\tLoss: 0.5259\n",
      "Train Epoch: 31 [1000/3200 (31.25%)]\tLoss: 0.5605\n",
      "Train Epoch: 31 [1200/3200 (37.5%)]\tLoss: 0.5253\n",
      "Train Epoch: 31 [1400/3200 (43.75%)]\tLoss: 0.5295\n",
      "Train Epoch: 31 [1600/3200 (50.0%)]\tLoss: 0.3673\n",
      "Train Epoch: 31 [1800/3200 (56.25%)]\tLoss: 0.5317\n",
      "Train Epoch: 31 [2000/3200 (62.5%)]\tLoss: 0.4308\n",
      "Train Epoch: 31 [2200/3200 (68.75%)]\tLoss: 0.5248\n",
      "Train Epoch: 31 [2400/3200 (75.0%)]\tLoss: 0.5289\n",
      "Train Epoch: 31 [2600/3200 (81.25%)]\tLoss: 0.5262\n",
      "Train Epoch: 31 [2800/3200 (87.5%)]\tLoss: 0.5342\n",
      "Train Epoch: 31 [3000/3200 (93.75%)]\tLoss: 0.4894\n",
      "Train Epoch: 31 [3200/3200 (100.0%)]\tLoss: 0.5315\n",
      "Done epoch #31, time for this epoch: 315.0831482410431s\n",
      "Start epoch #32, learning rate for this epoch: [5.59872e-06]\n",
      "Train Epoch: 32 [200/3200 (6.25%)]\tLoss: 0.5620\n",
      "Train Epoch: 32 [400/3200 (12.5%)]\tLoss: 0.5274\n",
      "Train Epoch: 32 [600/3200 (18.75%)]\tLoss: 0.5323\n",
      "Train Epoch: 32 [800/3200 (25.0%)]\tLoss: 0.5284\n",
      "Train Epoch: 32 [1000/3200 (31.25%)]\tLoss: 0.4909\n",
      "Train Epoch: 32 [1200/3200 (37.5%)]\tLoss: 0.5233\n",
      "Train Epoch: 32 [1400/3200 (43.75%)]\tLoss: 0.5270\n",
      "Train Epoch: 32 [1600/3200 (50.0%)]\tLoss: 0.2653\n",
      "Train Epoch: 32 [1800/3200 (56.25%)]\tLoss: 0.5650\n",
      "Train Epoch: 32 [2000/3200 (62.5%)]\tLoss: 0.4414\n",
      "Train Epoch: 32 [2200/3200 (68.75%)]\tLoss: 0.4889\n",
      "Train Epoch: 32 [2400/3200 (75.0%)]\tLoss: 0.5353\n",
      "Train Epoch: 32 [2600/3200 (81.25%)]\tLoss: 0.4944\n",
      "Train Epoch: 32 [2800/3200 (87.5%)]\tLoss: 0.5259\n",
      "Train Epoch: 32 [3000/3200 (93.75%)]\tLoss: 0.5269\n",
      "Train Epoch: 32 [3200/3200 (100.0%)]\tLoss: 0.5624\n",
      "Done epoch #32, time for this epoch: 314.13349294662476s\n",
      "Start epoch #33, learning rate for this epoch: [3.359232e-06]\n",
      "Train Epoch: 33 [200/3200 (6.25%)]\tLoss: 0.4972\n",
      "Train Epoch: 33 [400/3200 (12.5%)]\tLoss: 0.5659\n",
      "Train Epoch: 33 [600/3200 (18.75%)]\tLoss: 0.5283\n",
      "Train Epoch: 33 [800/3200 (25.0%)]\tLoss: 0.4901\n",
      "Train Epoch: 33 [1000/3200 (31.25%)]\tLoss: 0.4928\n",
      "Train Epoch: 33 [1200/3200 (37.5%)]\tLoss: 0.4613\n",
      "Train Epoch: 33 [1400/3200 (43.75%)]\tLoss: 0.5326\n",
      "Train Epoch: 33 [1600/3200 (50.0%)]\tLoss: 0.5603\n",
      "Train Epoch: 33 [1800/3200 (56.25%)]\tLoss: 0.3993\n",
      "Train Epoch: 33 [2000/3200 (62.5%)]\tLoss: 0.5659\n",
      "Train Epoch: 33 [2200/3200 (68.75%)]\tLoss: 0.4017\n",
      "Train Epoch: 33 [2400/3200 (75.0%)]\tLoss: 0.4971\n",
      "Train Epoch: 33 [2600/3200 (81.25%)]\tLoss: 0.5283\n",
      "Train Epoch: 33 [2800/3200 (87.5%)]\tLoss: 0.4872\n",
      "Train Epoch: 33 [3000/3200 (93.75%)]\tLoss: 0.5650\n",
      "Train Epoch: 33 [3200/3200 (100.0%)]\tLoss: 0.4319\n",
      "Done epoch #33, time for this epoch: 312.0713837146759s\n",
      "Start epoch #34, learning rate for this epoch: [3.359232e-06]\n",
      "Train Epoch: 34 [200/3200 (6.25%)]\tLoss: 0.5624\n",
      "Train Epoch: 34 [400/3200 (12.5%)]\tLoss: 0.5271\n",
      "Train Epoch: 34 [600/3200 (18.75%)]\tLoss: 0.4355\n",
      "Train Epoch: 34 [800/3200 (25.0%)]\tLoss: 0.5257\n",
      "Train Epoch: 34 [1000/3200 (31.25%)]\tLoss: 0.5267\n",
      "Train Epoch: 34 [1200/3200 (37.5%)]\tLoss: 0.4011\n",
      "Train Epoch: 34 [1400/3200 (43.75%)]\tLoss: 0.5642\n",
      "Train Epoch: 34 [1600/3200 (50.0%)]\tLoss: 0.5292\n",
      "Train Epoch: 34 [1800/3200 (56.25%)]\tLoss: 0.5257\n",
      "Train Epoch: 34 [2000/3200 (62.5%)]\tLoss: 0.5791\n",
      "Train Epoch: 34 [2200/3200 (68.75%)]\tLoss: 0.4980\n",
      "Train Epoch: 34 [2400/3200 (75.0%)]\tLoss: 0.5359\n",
      "Train Epoch: 34 [2600/3200 (81.25%)]\tLoss: 0.5273\n",
      "Train Epoch: 34 [2800/3200 (87.5%)]\tLoss: 0.5354\n",
      "Train Epoch: 34 [3000/3200 (93.75%)]\tLoss: 0.5276\n",
      "Train Epoch: 34 [3200/3200 (100.0%)]\tLoss: 0.4898\n",
      "Done epoch #34, time for this epoch: 314.44452810287476s\n",
      "Start epoch #35, learning rate for this epoch: [3.359232e-06]\n",
      "Train Epoch: 35 [200/3200 (6.25%)]\tLoss: 0.5656\n",
      "Train Epoch: 35 [400/3200 (12.5%)]\tLoss: 0.5671\n",
      "Train Epoch: 35 [600/3200 (18.75%)]\tLoss: 0.5067\n",
      "Train Epoch: 35 [800/3200 (25.0%)]\tLoss: 0.5266\n",
      "Train Epoch: 35 [1000/3200 (31.25%)]\tLoss: 0.5609\n",
      "Train Epoch: 35 [1200/3200 (37.5%)]\tLoss: 0.5338\n",
      "Train Epoch: 35 [1400/3200 (43.75%)]\tLoss: 0.5744\n",
      "Train Epoch: 35 [1600/3200 (50.0%)]\tLoss: 0.5068\n",
      "Train Epoch: 35 [1800/3200 (56.25%)]\tLoss: 0.4874\n",
      "Train Epoch: 35 [2000/3200 (62.5%)]\tLoss: 0.5678\n",
      "Train Epoch: 35 [2200/3200 (68.75%)]\tLoss: 0.4940\n",
      "Train Epoch: 35 [2400/3200 (75.0%)]\tLoss: 0.4393\n",
      "Train Epoch: 35 [2600/3200 (81.25%)]\tLoss: 0.5422\n",
      "Train Epoch: 35 [2800/3200 (87.5%)]\tLoss: 0.5285\n",
      "Train Epoch: 35 [3000/3200 (93.75%)]\tLoss: 0.4360\n",
      "Train Epoch: 35 [3200/3200 (100.0%)]\tLoss: 0.5638\n",
      "Done epoch #35, time for this epoch: 313.8296003341675s\n",
      "Start epoch #36, learning rate for this epoch: [3.359232e-06]\n",
      "Train Epoch: 36 [200/3200 (6.25%)]\tLoss: 0.5291\n",
      "Train Epoch: 36 [400/3200 (12.5%)]\tLoss: 0.4971\n",
      "Train Epoch: 36 [600/3200 (18.75%)]\tLoss: 0.5252\n",
      "Train Epoch: 36 [800/3200 (25.0%)]\tLoss: 0.4901\n",
      "Train Epoch: 36 [1000/3200 (31.25%)]\tLoss: 0.5619\n",
      "Train Epoch: 36 [1200/3200 (37.5%)]\tLoss: 0.4970\n",
      "Train Epoch: 36 [1400/3200 (43.75%)]\tLoss: 0.4888\n",
      "Train Epoch: 36 [1600/3200 (50.0%)]\tLoss: 0.5265\n",
      "Train Epoch: 36 [1800/3200 (56.25%)]\tLoss: 0.5275\n",
      "Train Epoch: 36 [2000/3200 (62.5%)]\tLoss: 0.5671\n",
      "Train Epoch: 36 [2200/3200 (68.75%)]\tLoss: 0.4102\n",
      "Train Epoch: 36 [2400/3200 (75.0%)]\tLoss: 0.5670\n",
      "Train Epoch: 36 [2600/3200 (81.25%)]\tLoss: 0.5266\n",
      "Train Epoch: 36 [2800/3200 (87.5%)]\tLoss: 0.4048\n",
      "Train Epoch: 36 [3000/3200 (93.75%)]\tLoss: 0.4922\n",
      "Train Epoch: 36 [3200/3200 (100.0%)]\tLoss: 0.5621\n",
      "Done epoch #36, time for this epoch: 314.1131899356842s\n",
      "Start epoch #37, learning rate for this epoch: [2.0155392e-06]\n",
      "Train Epoch: 37 [200/3200 (6.25%)]\tLoss: 0.5037\n",
      "Train Epoch: 37 [400/3200 (12.5%)]\tLoss: 0.5636\n",
      "Train Epoch: 37 [600/3200 (18.75%)]\tLoss: 0.5379\n",
      "Train Epoch: 37 [800/3200 (25.0%)]\tLoss: 0.5347\n",
      "Train Epoch: 37 [1000/3200 (31.25%)]\tLoss: 0.5287\n",
      "Train Epoch: 37 [1200/3200 (37.5%)]\tLoss: 0.3592\n",
      "Train Epoch: 37 [1400/3200 (43.75%)]\tLoss: 0.4143\n",
      "Train Epoch: 37 [1600/3200 (50.0%)]\tLoss: 0.3975\n",
      "Train Epoch: 37 [1800/3200 (56.25%)]\tLoss: 0.5630\n",
      "Train Epoch: 37 [2000/3200 (62.5%)]\tLoss: 0.5261\n",
      "Train Epoch: 37 [2200/3200 (68.75%)]\tLoss: 0.5246\n",
      "Train Epoch: 37 [2400/3200 (75.0%)]\tLoss: 0.5262\n",
      "Train Epoch: 37 [2600/3200 (81.25%)]\tLoss: 0.5649\n",
      "Train Epoch: 37 [2800/3200 (87.5%)]\tLoss: 0.5624\n",
      "Train Epoch: 37 [3000/3200 (93.75%)]\tLoss: 0.5273\n",
      "Train Epoch: 37 [3200/3200 (100.0%)]\tLoss: 0.5638\n",
      "Done epoch #37, time for this epoch: 314.56741285324097s\n",
      "Start epoch #38, learning rate for this epoch: [2.0155392e-06]\n",
      "Train Epoch: 38 [200/3200 (6.25%)]\tLoss: 0.5728\n",
      "Train Epoch: 38 [400/3200 (12.5%)]\tLoss: 0.4323\n",
      "Train Epoch: 38 [600/3200 (18.75%)]\tLoss: 0.5277\n",
      "Train Epoch: 38 [800/3200 (25.0%)]\tLoss: 0.3730\n",
      "Train Epoch: 38 [1000/3200 (31.25%)]\tLoss: 0.5276\n",
      "Train Epoch: 38 [1200/3200 (37.5%)]\tLoss: 0.4911\n",
      "Train Epoch: 38 [1400/3200 (43.75%)]\tLoss: 0.4561\n",
      "Train Epoch: 38 [1600/3200 (50.0%)]\tLoss: 0.5240\n",
      "Train Epoch: 38 [1800/3200 (56.25%)]\tLoss: 0.2742\n",
      "Train Epoch: 38 [2000/3200 (62.5%)]\tLoss: 0.5618\n",
      "Train Epoch: 38 [2200/3200 (68.75%)]\tLoss: 0.5646\n",
      "Train Epoch: 38 [2400/3200 (75.0%)]\tLoss: 0.4991\n",
      "Train Epoch: 38 [2600/3200 (81.25%)]\tLoss: 0.5628\n",
      "Train Epoch: 38 [2800/3200 (87.5%)]\tLoss: 0.5363\n",
      "Train Epoch: 38 [3000/3200 (93.75%)]\tLoss: 0.5674\n",
      "Train Epoch: 38 [3200/3200 (100.0%)]\tLoss: 0.4555\n",
      "Done epoch #38, time for this epoch: 315.4465579986572s\n",
      "Start epoch #39, learning rate for this epoch: [2.0155392e-06]\n",
      "Train Epoch: 39 [200/3200 (6.25%)]\tLoss: 0.4886\n",
      "Train Epoch: 39 [400/3200 (12.5%)]\tLoss: 0.5647\n",
      "Train Epoch: 39 [600/3200 (18.75%)]\tLoss: 0.4876\n",
      "Train Epoch: 39 [800/3200 (25.0%)]\tLoss: 0.4869\n",
      "Train Epoch: 39 [1000/3200 (31.25%)]\tLoss: 0.5262\n",
      "Train Epoch: 39 [1200/3200 (37.5%)]\tLoss: 0.5271\n",
      "Train Epoch: 39 [1400/3200 (43.75%)]\tLoss: 0.5634\n",
      "Train Epoch: 39 [1600/3200 (50.0%)]\tLoss: 0.4296\n",
      "Train Epoch: 39 [1800/3200 (56.25%)]\tLoss: 0.4940\n",
      "Train Epoch: 39 [2000/3200 (62.5%)]\tLoss: 0.4992\n",
      "Train Epoch: 39 [2200/3200 (68.75%)]\tLoss: 0.5369\n",
      "Train Epoch: 39 [2400/3200 (75.0%)]\tLoss: 0.5297\n",
      "Train Epoch: 39 [2600/3200 (81.25%)]\tLoss: 0.2712\n",
      "Train Epoch: 39 [2800/3200 (87.5%)]\tLoss: 0.3305\n",
      "Train Epoch: 39 [3000/3200 (93.75%)]\tLoss: 0.4927\n",
      "Train Epoch: 39 [3200/3200 (100.0%)]\tLoss: 0.5600\n",
      "Done epoch #39, time for this epoch: 311.67661356925964s\n",
      "Start epoch #40, learning rate for this epoch: [2.0155392e-06]\n",
      "Train Epoch: 40 [200/3200 (6.25%)]\tLoss: 0.4970\n",
      "Train Epoch: 40 [400/3200 (12.5%)]\tLoss: 0.4951\n",
      "Train Epoch: 40 [600/3200 (18.75%)]\tLoss: 0.5295\n",
      "Train Epoch: 40 [800/3200 (25.0%)]\tLoss: 0.5677\n",
      "Train Epoch: 40 [1000/3200 (31.25%)]\tLoss: 0.5288\n",
      "Train Epoch: 40 [1200/3200 (37.5%)]\tLoss: 0.5043\n",
      "Train Epoch: 40 [1400/3200 (43.75%)]\tLoss: 0.4930\n",
      "Train Epoch: 40 [1600/3200 (50.0%)]\tLoss: 0.4881\n",
      "Train Epoch: 40 [1800/3200 (56.25%)]\tLoss: 0.5289\n",
      "Train Epoch: 40 [2000/3200 (62.5%)]\tLoss: 0.5371\n",
      "Train Epoch: 40 [2200/3200 (68.75%)]\tLoss: 0.5280\n",
      "Train Epoch: 40 [2400/3200 (75.0%)]\tLoss: 0.4918\n",
      "Train Epoch: 40 [2600/3200 (81.25%)]\tLoss: 0.5661\n",
      "Train Epoch: 40 [2800/3200 (87.5%)]\tLoss: 0.5237\n",
      "Train Epoch: 40 [3000/3200 (93.75%)]\tLoss: 0.4634\n",
      "Train Epoch: 40 [3200/3200 (100.0%)]\tLoss: 0.5277\n",
      "Done epoch #40, time for this epoch: 314.0050642490387s\n",
      "Start epoch #41, learning rate for this epoch: [1.20932352e-06]\n",
      "Train Epoch: 41 [200/3200 (6.25%)]\tLoss: 0.5344\n",
      "Train Epoch: 41 [400/3200 (12.5%)]\tLoss: 0.5634\n",
      "Train Epoch: 41 [600/3200 (18.75%)]\tLoss: 0.5296\n",
      "Train Epoch: 41 [800/3200 (25.0%)]\tLoss: 0.5605\n",
      "Train Epoch: 41 [1000/3200 (31.25%)]\tLoss: 0.5308\n",
      "Train Epoch: 41 [1200/3200 (37.5%)]\tLoss: 0.4336\n",
      "Train Epoch: 41 [1400/3200 (43.75%)]\tLoss: 0.5054\n",
      "Train Epoch: 41 [1600/3200 (50.0%)]\tLoss: 0.4985\n",
      "Train Epoch: 41 [1800/3200 (56.25%)]\tLoss: 0.4876\n",
      "Train Epoch: 41 [2000/3200 (62.5%)]\tLoss: 0.4934\n",
      "Train Epoch: 41 [2200/3200 (68.75%)]\tLoss: 0.5621\n",
      "Train Epoch: 41 [2400/3200 (75.0%)]\tLoss: 0.5341\n",
      "Train Epoch: 41 [2600/3200 (81.25%)]\tLoss: 0.4146\n",
      "Train Epoch: 41 [2800/3200 (87.5%)]\tLoss: 0.5622\n",
      "Train Epoch: 41 [3000/3200 (93.75%)]\tLoss: 0.3980\n",
      "Train Epoch: 41 [3200/3200 (100.0%)]\tLoss: 0.4974\n",
      "Done epoch #41, time for this epoch: 316.67043471336365s\n",
      "Start epoch #42, learning rate for this epoch: [1.20932352e-06]\n",
      "Train Epoch: 42 [200/3200 (6.25%)]\tLoss: 0.4910\n",
      "Train Epoch: 42 [400/3200 (12.5%)]\tLoss: 0.5242\n",
      "Train Epoch: 42 [600/3200 (18.75%)]\tLoss: 0.5282\n",
      "Train Epoch: 42 [800/3200 (25.0%)]\tLoss: 0.3604\n",
      "Train Epoch: 42 [1000/3200 (31.25%)]\tLoss: 0.4858\n",
      "Train Epoch: 42 [1200/3200 (37.5%)]\tLoss: 0.5383\n",
      "Train Epoch: 42 [1400/3200 (43.75%)]\tLoss: 0.5256\n",
      "Train Epoch: 42 [1600/3200 (50.0%)]\tLoss: 0.5285\n",
      "Train Epoch: 42 [1800/3200 (56.25%)]\tLoss: 0.5441\n",
      "Train Epoch: 42 [2000/3200 (62.5%)]\tLoss: 0.5294\n",
      "Train Epoch: 42 [2200/3200 (68.75%)]\tLoss: 0.5645\n",
      "Train Epoch: 42 [2400/3200 (75.0%)]\tLoss: 0.5263\n",
      "Train Epoch: 42 [2600/3200 (81.25%)]\tLoss: 0.3964\n",
      "Train Epoch: 42 [2800/3200 (87.5%)]\tLoss: 0.4924\n",
      "Train Epoch: 42 [3000/3200 (93.75%)]\tLoss: 0.4967\n",
      "Train Epoch: 42 [3200/3200 (100.0%)]\tLoss: 0.5287\n",
      "Done epoch #42, time for this epoch: 316.2526206970215s\n",
      "Start epoch #43, learning rate for this epoch: [1.20932352e-06]\n",
      "Train Epoch: 43 [200/3200 (6.25%)]\tLoss: 0.5642\n",
      "Train Epoch: 43 [400/3200 (12.5%)]\tLoss: 0.5686\n",
      "Train Epoch: 43 [600/3200 (18.75%)]\tLoss: 0.3728\n",
      "Train Epoch: 43 [800/3200 (25.0%)]\tLoss: 0.4858\n",
      "Train Epoch: 43 [1000/3200 (31.25%)]\tLoss: 0.5274\n",
      "Train Epoch: 43 [1200/3200 (37.5%)]\tLoss: 0.5238\n",
      "Train Epoch: 43 [1400/3200 (43.75%)]\tLoss: 0.5276\n",
      "Train Epoch: 43 [1600/3200 (50.0%)]\tLoss: 0.5316\n",
      "Train Epoch: 43 [1800/3200 (56.25%)]\tLoss: 0.3951\n",
      "Train Epoch: 43 [2000/3200 (62.5%)]\tLoss: 0.5290\n",
      "Train Epoch: 43 [2200/3200 (68.75%)]\tLoss: 0.5006\n",
      "Train Epoch: 43 [2400/3200 (75.0%)]\tLoss: 0.5400\n",
      "Train Epoch: 43 [2600/3200 (81.25%)]\tLoss: 0.4342\n",
      "Train Epoch: 43 [2800/3200 (87.5%)]\tLoss: 0.5657\n",
      "Train Epoch: 43 [3000/3200 (93.75%)]\tLoss: 0.5277\n",
      "Train Epoch: 43 [3200/3200 (100.0%)]\tLoss: 0.4123\n",
      "Done epoch #43, time for this epoch: 316.2458212375641s\n",
      "Start epoch #44, learning rate for this epoch: [1.20932352e-06]\n",
      "Train Epoch: 44 [200/3200 (6.25%)]\tLoss: 0.4894\n",
      "Train Epoch: 44 [400/3200 (12.5%)]\tLoss: 0.4863\n",
      "Train Epoch: 44 [600/3200 (18.75%)]\tLoss: 0.4064\n",
      "Train Epoch: 44 [800/3200 (25.0%)]\tLoss: 0.5724\n",
      "Train Epoch: 44 [1000/3200 (31.25%)]\tLoss: 0.5296\n",
      "Train Epoch: 44 [1200/3200 (37.5%)]\tLoss: 0.4981\n",
      "Train Epoch: 44 [1400/3200 (43.75%)]\tLoss: 0.4906\n",
      "Train Epoch: 44 [1600/3200 (50.0%)]\tLoss: 0.5262\n",
      "Train Epoch: 44 [1800/3200 (56.25%)]\tLoss: 0.5623\n",
      "Train Epoch: 44 [2000/3200 (62.5%)]\tLoss: 0.5674\n",
      "Train Epoch: 44 [2200/3200 (68.75%)]\tLoss: 0.3745\n",
      "Train Epoch: 44 [2400/3200 (75.0%)]\tLoss: 0.4989\n",
      "Train Epoch: 44 [2600/3200 (81.25%)]\tLoss: 0.5290\n",
      "Train Epoch: 44 [2800/3200 (87.5%)]\tLoss: 0.4909\n",
      "Train Epoch: 44 [3000/3200 (93.75%)]\tLoss: 0.4638\n",
      "Train Epoch: 44 [3200/3200 (100.0%)]\tLoss: 0.4280\n",
      "Done epoch #44, time for this epoch: 316.14068484306335s\n",
      "Start epoch #45, learning rate for this epoch: [7.25594112e-07]\n",
      "Train Epoch: 45 [200/3200 (6.25%)]\tLoss: 0.5269\n",
      "Train Epoch: 45 [400/3200 (12.5%)]\tLoss: 0.5010\n",
      "Train Epoch: 45 [600/3200 (18.75%)]\tLoss: 0.4920\n",
      "Train Epoch: 45 [800/3200 (25.0%)]\tLoss: 0.4572\n",
      "Train Epoch: 45 [1000/3200 (31.25%)]\tLoss: 0.2681\n",
      "Train Epoch: 45 [1200/3200 (37.5%)]\tLoss: 0.5623\n",
      "Train Epoch: 45 [1400/3200 (43.75%)]\tLoss: 0.4924\n",
      "Train Epoch: 45 [1600/3200 (50.0%)]\tLoss: 0.5363\n",
      "Train Epoch: 45 [1800/3200 (56.25%)]\tLoss: 0.5236\n",
      "Train Epoch: 45 [2000/3200 (62.5%)]\tLoss: 0.4939\n",
      "Train Epoch: 45 [2200/3200 (68.75%)]\tLoss: 0.4925\n",
      "Train Epoch: 45 [2400/3200 (75.0%)]\tLoss: 0.5684\n",
      "Train Epoch: 45 [2600/3200 (81.25%)]\tLoss: 0.5260\n",
      "Train Epoch: 45 [2800/3200 (87.5%)]\tLoss: 0.5619\n",
      "Train Epoch: 45 [3000/3200 (93.75%)]\tLoss: 0.4892\n",
      "Train Epoch: 45 [3200/3200 (100.0%)]\tLoss: 0.5277\n",
      "Done epoch #45, time for this epoch: 313.0873022079468s\n",
      "Start epoch #46, learning rate for this epoch: [7.25594112e-07]\n",
      "Train Epoch: 46 [200/3200 (6.25%)]\tLoss: 0.4908\n",
      "Train Epoch: 46 [400/3200 (12.5%)]\tLoss: 0.5290\n",
      "Train Epoch: 46 [600/3200 (18.75%)]\tLoss: 0.3991\n",
      "Train Epoch: 46 [800/3200 (25.0%)]\tLoss: 0.4864\n",
      "Train Epoch: 46 [1000/3200 (31.25%)]\tLoss: 0.5287\n",
      "Train Epoch: 46 [1200/3200 (37.5%)]\tLoss: 0.3971\n",
      "Train Epoch: 46 [1400/3200 (43.75%)]\tLoss: 0.3968\n",
      "Train Epoch: 46 [1600/3200 (50.0%)]\tLoss: 0.4320\n",
      "Train Epoch: 46 [1800/3200 (56.25%)]\tLoss: 0.4961\n",
      "Train Epoch: 46 [2000/3200 (62.5%)]\tLoss: 0.4928\n",
      "Train Epoch: 46 [2200/3200 (68.75%)]\tLoss: 0.5632\n",
      "Train Epoch: 46 [2400/3200 (75.0%)]\tLoss: 0.5641\n",
      "Train Epoch: 46 [2600/3200 (81.25%)]\tLoss: 0.4972\n",
      "Train Epoch: 46 [2800/3200 (87.5%)]\tLoss: 0.2920\n",
      "Train Epoch: 46 [3000/3200 (93.75%)]\tLoss: 0.5340\n",
      "Train Epoch: 46 [3200/3200 (100.0%)]\tLoss: 0.5321\n",
      "Done epoch #46, time for this epoch: 313.25584268569946s\n",
      "Start epoch #47, learning rate for this epoch: [7.25594112e-07]\n",
      "Train Epoch: 47 [200/3200 (6.25%)]\tLoss: 0.5613\n",
      "Train Epoch: 47 [400/3200 (12.5%)]\tLoss: 0.5603\n",
      "Train Epoch: 47 [600/3200 (18.75%)]\tLoss: 0.5260\n",
      "Train Epoch: 47 [800/3200 (25.0%)]\tLoss: 0.4296\n",
      "Train Epoch: 47 [1000/3200 (31.25%)]\tLoss: 0.5274\n",
      "Train Epoch: 47 [1200/3200 (37.5%)]\tLoss: 0.4907\n",
      "Train Epoch: 47 [1400/3200 (43.75%)]\tLoss: 0.5292\n",
      "Train Epoch: 47 [1600/3200 (50.0%)]\tLoss: 0.5274\n",
      "Train Epoch: 47 [1800/3200 (56.25%)]\tLoss: 0.5282\n",
      "Train Epoch: 47 [2000/3200 (62.5%)]\tLoss: 0.5322\n",
      "Train Epoch: 47 [2200/3200 (68.75%)]\tLoss: 0.4200\n",
      "Train Epoch: 47 [2400/3200 (75.0%)]\tLoss: 0.3680\n",
      "Train Epoch: 47 [2600/3200 (81.25%)]\tLoss: 0.5268\n",
      "Train Epoch: 47 [2800/3200 (87.5%)]\tLoss: 0.4924\n",
      "Train Epoch: 47 [3000/3200 (93.75%)]\tLoss: 0.5325\n",
      "Train Epoch: 47 [3200/3200 (100.0%)]\tLoss: 0.5259\n",
      "Done epoch #47, time for this epoch: 315.5733346939087s\n",
      "Start epoch #48, learning rate for this epoch: [7.25594112e-07]\n",
      "Train Epoch: 48 [200/3200 (6.25%)]\tLoss: 0.5646\n",
      "Train Epoch: 48 [400/3200 (12.5%)]\tLoss: 0.4914\n",
      "Train Epoch: 48 [600/3200 (18.75%)]\tLoss: 0.4902\n",
      "Train Epoch: 48 [800/3200 (25.0%)]\tLoss: 0.5583\n",
      "Train Epoch: 48 [1000/3200 (31.25%)]\tLoss: 0.3959\n",
      "Train Epoch: 48 [1200/3200 (37.5%)]\tLoss: 0.5354\n",
      "Train Epoch: 48 [1400/3200 (43.75%)]\tLoss: 0.5232\n",
      "Train Epoch: 48 [1600/3200 (50.0%)]\tLoss: 0.5618\n",
      "Train Epoch: 48 [1800/3200 (56.25%)]\tLoss: 0.3918\n",
      "Train Epoch: 48 [2000/3200 (62.5%)]\tLoss: 0.5636\n",
      "Train Epoch: 48 [2200/3200 (68.75%)]\tLoss: 0.3929\n",
      "Train Epoch: 48 [2400/3200 (75.0%)]\tLoss: 0.3921\n",
      "Train Epoch: 48 [2600/3200 (81.25%)]\tLoss: 0.5262\n",
      "Train Epoch: 48 [2800/3200 (87.5%)]\tLoss: 0.5668\n",
      "Train Epoch: 48 [3000/3200 (93.75%)]\tLoss: 0.5308\n",
      "Train Epoch: 48 [3200/3200 (100.0%)]\tLoss: 0.5309\n",
      "Done epoch #48, time for this epoch: 315.4373791217804s\n",
      "Start epoch #49, learning rate for this epoch: [4.353564672e-07]\n",
      "Train Epoch: 49 [200/3200 (6.25%)]\tLoss: 0.5393\n",
      "Train Epoch: 49 [400/3200 (12.5%)]\tLoss: 0.5238\n",
      "Train Epoch: 49 [600/3200 (18.75%)]\tLoss: 0.5597\n",
      "Train Epoch: 49 [800/3200 (25.0%)]\tLoss: 0.5259\n",
      "Train Epoch: 49 [1000/3200 (31.25%)]\tLoss: 0.5239\n",
      "Train Epoch: 49 [1200/3200 (37.5%)]\tLoss: 0.5260\n",
      "Train Epoch: 49 [1400/3200 (43.75%)]\tLoss: 0.4911\n",
      "Train Epoch: 49 [1600/3200 (50.0%)]\tLoss: 0.5324\n",
      "Train Epoch: 49 [1800/3200 (56.25%)]\tLoss: 0.3911\n",
      "Train Epoch: 49 [2000/3200 (62.5%)]\tLoss: 0.5279\n",
      "Train Epoch: 49 [2200/3200 (68.75%)]\tLoss: 0.5240\n",
      "Train Epoch: 49 [2400/3200 (75.0%)]\tLoss: 0.5100\n",
      "Train Epoch: 49 [2600/3200 (81.25%)]\tLoss: 0.5621\n",
      "Train Epoch: 49 [2800/3200 (87.5%)]\tLoss: 0.5658\n",
      "Train Epoch: 49 [3000/3200 (93.75%)]\tLoss: 0.5267\n",
      "Train Epoch: 49 [3200/3200 (100.0%)]\tLoss: 0.5227\n",
      "Done epoch #49, time for this epoch: 311.9475531578064s\n",
      "Start epoch #50, learning rate for this epoch: [4.353564672e-07]\n",
      "Train Epoch: 50 [200/3200 (6.25%)]\tLoss: 0.5624\n",
      "Train Epoch: 50 [400/3200 (12.5%)]\tLoss: 0.4315\n",
      "Train Epoch: 50 [600/3200 (18.75%)]\tLoss: 0.5649\n",
      "Train Epoch: 50 [800/3200 (25.0%)]\tLoss: 0.5265\n",
      "Train Epoch: 50 [1000/3200 (31.25%)]\tLoss: 0.4331\n",
      "Train Epoch: 50 [1200/3200 (37.5%)]\tLoss: 0.5424\n",
      "Train Epoch: 50 [1400/3200 (43.75%)]\tLoss: 0.5053\n",
      "Train Epoch: 50 [1600/3200 (50.0%)]\tLoss: 0.4358\n",
      "Train Epoch: 50 [1800/3200 (56.25%)]\tLoss: 0.5640\n",
      "Train Epoch: 50 [2000/3200 (62.5%)]\tLoss: 0.4897\n",
      "Train Epoch: 50 [2200/3200 (68.75%)]\tLoss: 0.4504\n",
      "Train Epoch: 50 [2400/3200 (75.0%)]\tLoss: 0.4541\n",
      "Train Epoch: 50 [2600/3200 (81.25%)]\tLoss: 0.5288\n",
      "Train Epoch: 50 [2800/3200 (87.5%)]\tLoss: 0.5267\n",
      "Train Epoch: 50 [3000/3200 (93.75%)]\tLoss: 0.5302\n",
      "Train Epoch: 50 [3200/3200 (100.0%)]\tLoss: 0.4913\n",
      "Done epoch #50, time for this epoch: 315.06388330459595s\n"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project = \"assignment_3\"\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "train_loss_array = []\n",
    "test_loss_array = []\n",
    "last_loss = 9999999999999\n",
    "for epoch in range(epochs):\n",
    "    train_loss_epoch = 0\n",
    "    test_loss_epoch = 0\n",
    "    (train_loss_epoch, test_loss_epoch) = train(train_loader, \n",
    "                                              valid_loader, \n",
    "                                              learing_rate_scheduler, epoch, display_step)\n",
    "    \n",
    "    if test_loss_epoch < last_loss:\n",
    "        save_model(model, optimizer, checkpoint_path)\n",
    "        last_loss = test_loss_epoch\n",
    "        \n",
    "    learing_rate_scheduler.step()\n",
    "    train_loss_array.append(train_loss_epoch)\n",
    "    test_loss_array.append(test_loss_epoch)\n",
    "    wandb.log({\"Train loss\": train_loss_epoch, \"Valid loss\": test_loss_epoch})\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 2715462,
     "sourceId": 30892,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30580,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18530.560339,
   "end_time": "2023-11-16T07:28:56.181951",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-16T02:20:05.621612",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
